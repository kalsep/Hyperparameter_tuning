{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "039f0ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b5187d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c8c184d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "983f68b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(cancer.data, columns=cancer.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ba73a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Result'] = cancer.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a5888f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>radius error</th>\n",
       "      <th>texture error</th>\n",
       "      <th>perimeter error</th>\n",
       "      <th>area error</th>\n",
       "      <th>smoothness error</th>\n",
       "      <th>compactness error</th>\n",
       "      <th>concavity error</th>\n",
       "      <th>concave points error</th>\n",
       "      <th>symmetry error</th>\n",
       "      <th>fractal dimension error</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>1.0950</td>\n",
       "      <td>0.9053</td>\n",
       "      <td>8.589</td>\n",
       "      <td>153.40</td>\n",
       "      <td>0.006399</td>\n",
       "      <td>0.04904</td>\n",
       "      <td>0.05373</td>\n",
       "      <td>0.01587</td>\n",
       "      <td>0.03003</td>\n",
       "      <td>0.006193</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>0.5435</td>\n",
       "      <td>0.7339</td>\n",
       "      <td>3.398</td>\n",
       "      <td>74.08</td>\n",
       "      <td>0.005225</td>\n",
       "      <td>0.01308</td>\n",
       "      <td>0.01860</td>\n",
       "      <td>0.01340</td>\n",
       "      <td>0.01389</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>0.7456</td>\n",
       "      <td>0.7869</td>\n",
       "      <td>4.585</td>\n",
       "      <td>94.03</td>\n",
       "      <td>0.006150</td>\n",
       "      <td>0.04006</td>\n",
       "      <td>0.03832</td>\n",
       "      <td>0.02058</td>\n",
       "      <td>0.02250</td>\n",
       "      <td>0.004571</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>0.4956</td>\n",
       "      <td>1.1560</td>\n",
       "      <td>3.445</td>\n",
       "      <td>27.23</td>\n",
       "      <td>0.009110</td>\n",
       "      <td>0.07458</td>\n",
       "      <td>0.05661</td>\n",
       "      <td>0.01867</td>\n",
       "      <td>0.05963</td>\n",
       "      <td>0.009208</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>0.7572</td>\n",
       "      <td>0.7813</td>\n",
       "      <td>5.438</td>\n",
       "      <td>94.44</td>\n",
       "      <td>0.011490</td>\n",
       "      <td>0.02461</td>\n",
       "      <td>0.05688</td>\n",
       "      <td>0.01885</td>\n",
       "      <td>0.01756</td>\n",
       "      <td>0.005115</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  radius error  texture error  perimeter error  \\\n",
       "0                 0.07871        1.0950         0.9053            8.589   \n",
       "1                 0.05667        0.5435         0.7339            3.398   \n",
       "2                 0.05999        0.7456         0.7869            4.585   \n",
       "3                 0.09744        0.4956         1.1560            3.445   \n",
       "4                 0.05883        0.7572         0.7813            5.438   \n",
       "\n",
       "   area error  smoothness error  compactness error  concavity error  \\\n",
       "0      153.40          0.006399            0.04904          0.05373   \n",
       "1       74.08          0.005225            0.01308          0.01860   \n",
       "2       94.03          0.006150            0.04006          0.03832   \n",
       "3       27.23          0.009110            0.07458          0.05661   \n",
       "4       94.44          0.011490            0.02461          0.05688   \n",
       "\n",
       "   concave points error  symmetry error  fractal dimension error  \\\n",
       "0               0.01587         0.03003                 0.006193   \n",
       "1               0.01340         0.01389                 0.003532   \n",
       "2               0.02058         0.02250                 0.004571   \n",
       "3               0.01867         0.05963                 0.009208   \n",
       "4               0.01885         0.01756                 0.005115   \n",
       "\n",
       "   worst radius  worst texture  worst perimeter  worst area  worst smoothness  \\\n",
       "0         25.38          17.33           184.60      2019.0            0.1622   \n",
       "1         24.99          23.41           158.80      1956.0            0.1238   \n",
       "2         23.57          25.53           152.50      1709.0            0.1444   \n",
       "3         14.91          26.50            98.87       567.7            0.2098   \n",
       "4         22.54          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   worst compactness  worst concavity  worst concave points  worst symmetry  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   worst fractal dimension  Result  \n",
       "0                  0.11890       0  \n",
       "1                  0.08902       0  \n",
       "2                  0.08758       0  \n",
       "3                  0.17300       0  \n",
       "4                  0.07678       0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8000d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 31 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   mean radius              569 non-null    float64\n",
      " 1   mean texture             569 non-null    float64\n",
      " 2   mean perimeter           569 non-null    float64\n",
      " 3   mean area                569 non-null    float64\n",
      " 4   mean smoothness          569 non-null    float64\n",
      " 5   mean compactness         569 non-null    float64\n",
      " 6   mean concavity           569 non-null    float64\n",
      " 7   mean concave points      569 non-null    float64\n",
      " 8   mean symmetry            569 non-null    float64\n",
      " 9   mean fractal dimension   569 non-null    float64\n",
      " 10  radius error             569 non-null    float64\n",
      " 11  texture error            569 non-null    float64\n",
      " 12  perimeter error          569 non-null    float64\n",
      " 13  area error               569 non-null    float64\n",
      " 14  smoothness error         569 non-null    float64\n",
      " 15  compactness error        569 non-null    float64\n",
      " 16  concavity error          569 non-null    float64\n",
      " 17  concave points error     569 non-null    float64\n",
      " 18  symmetry error           569 non-null    float64\n",
      " 19  fractal dimension error  569 non-null    float64\n",
      " 20  worst radius             569 non-null    float64\n",
      " 21  worst texture            569 non-null    float64\n",
      " 22  worst perimeter          569 non-null    float64\n",
      " 23  worst area               569 non-null    float64\n",
      " 24  worst smoothness         569 non-null    float64\n",
      " 25  worst compactness        569 non-null    float64\n",
      " 26  worst concavity          569 non-null    float64\n",
      " 27  worst concave points     569 non-null    float64\n",
      " 28  worst symmetry           569 non-null    float64\n",
      " 29  worst fractal dimension  569 non-null    float64\n",
      " 30  Result                   569 non-null    int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 137.9 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "748efb8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>radius error</th>\n",
       "      <th>texture error</th>\n",
       "      <th>perimeter error</th>\n",
       "      <th>area error</th>\n",
       "      <th>smoothness error</th>\n",
       "      <th>compactness error</th>\n",
       "      <th>concavity error</th>\n",
       "      <th>concave points error</th>\n",
       "      <th>symmetry error</th>\n",
       "      <th>fractal dimension error</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14.127292</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>0.062798</td>\n",
       "      <td>0.405172</td>\n",
       "      <td>1.216853</td>\n",
       "      <td>2.866059</td>\n",
       "      <td>40.337079</td>\n",
       "      <td>0.007041</td>\n",
       "      <td>0.025478</td>\n",
       "      <td>0.031894</td>\n",
       "      <td>0.011796</td>\n",
       "      <td>0.020542</td>\n",
       "      <td>0.003795</td>\n",
       "      <td>16.269190</td>\n",
       "      <td>25.677223</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.083946</td>\n",
       "      <td>0.627417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.524049</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>0.027414</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>0.277313</td>\n",
       "      <td>0.551648</td>\n",
       "      <td>2.021855</td>\n",
       "      <td>45.491006</td>\n",
       "      <td>0.003003</td>\n",
       "      <td>0.017908</td>\n",
       "      <td>0.030186</td>\n",
       "      <td>0.006170</td>\n",
       "      <td>0.008266</td>\n",
       "      <td>0.002646</td>\n",
       "      <td>4.833242</td>\n",
       "      <td>6.146258</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.065732</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.018061</td>\n",
       "      <td>0.483918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.981000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.049960</td>\n",
       "      <td>0.111500</td>\n",
       "      <td>0.360200</td>\n",
       "      <td>0.757000</td>\n",
       "      <td>6.802000</td>\n",
       "      <td>0.001713</td>\n",
       "      <td>0.002252</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007882</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>7.930000</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.055040</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.700000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>0.232400</td>\n",
       "      <td>0.833900</td>\n",
       "      <td>1.606000</td>\n",
       "      <td>17.850000</td>\n",
       "      <td>0.005169</td>\n",
       "      <td>0.013080</td>\n",
       "      <td>0.015090</td>\n",
       "      <td>0.007638</td>\n",
       "      <td>0.015160</td>\n",
       "      <td>0.002248</td>\n",
       "      <td>13.010000</td>\n",
       "      <td>21.080000</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.071460</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.370000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.324200</td>\n",
       "      <td>1.108000</td>\n",
       "      <td>2.287000</td>\n",
       "      <td>24.530000</td>\n",
       "      <td>0.006380</td>\n",
       "      <td>0.020450</td>\n",
       "      <td>0.025890</td>\n",
       "      <td>0.010930</td>\n",
       "      <td>0.018730</td>\n",
       "      <td>0.003187</td>\n",
       "      <td>14.970000</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.080040</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15.780000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.066120</td>\n",
       "      <td>0.478900</td>\n",
       "      <td>1.474000</td>\n",
       "      <td>3.357000</td>\n",
       "      <td>45.190000</td>\n",
       "      <td>0.008146</td>\n",
       "      <td>0.032450</td>\n",
       "      <td>0.042050</td>\n",
       "      <td>0.014710</td>\n",
       "      <td>0.023480</td>\n",
       "      <td>0.004558</td>\n",
       "      <td>18.790000</td>\n",
       "      <td>29.720000</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.092080</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>28.110000</td>\n",
       "      <td>39.280000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>0.097440</td>\n",
       "      <td>2.873000</td>\n",
       "      <td>4.885000</td>\n",
       "      <td>21.980000</td>\n",
       "      <td>542.200000</td>\n",
       "      <td>0.031130</td>\n",
       "      <td>0.135400</td>\n",
       "      <td>0.396000</td>\n",
       "      <td>0.052790</td>\n",
       "      <td>0.078950</td>\n",
       "      <td>0.029840</td>\n",
       "      <td>36.040000</td>\n",
       "      <td>49.540000</td>\n",
       "      <td>251.200000</td>\n",
       "      <td>4254.000000</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>1.058000</td>\n",
       "      <td>1.252000</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.207500</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean radius  mean texture  mean perimeter    mean area  \\\n",
       "count   569.000000    569.000000      569.000000   569.000000   \n",
       "mean     14.127292     19.289649       91.969033   654.889104   \n",
       "std       3.524049      4.301036       24.298981   351.914129   \n",
       "min       6.981000      9.710000       43.790000   143.500000   \n",
       "25%      11.700000     16.170000       75.170000   420.300000   \n",
       "50%      13.370000     18.840000       86.240000   551.100000   \n",
       "75%      15.780000     21.800000      104.100000   782.700000   \n",
       "max      28.110000     39.280000      188.500000  2501.000000   \n",
       "\n",
       "       mean smoothness  mean compactness  mean concavity  mean concave points  \\\n",
       "count       569.000000        569.000000      569.000000           569.000000   \n",
       "mean          0.096360          0.104341        0.088799             0.048919   \n",
       "std           0.014064          0.052813        0.079720             0.038803   \n",
       "min           0.052630          0.019380        0.000000             0.000000   \n",
       "25%           0.086370          0.064920        0.029560             0.020310   \n",
       "50%           0.095870          0.092630        0.061540             0.033500   \n",
       "75%           0.105300          0.130400        0.130700             0.074000   \n",
       "max           0.163400          0.345400        0.426800             0.201200   \n",
       "\n",
       "       mean symmetry  mean fractal dimension  radius error  texture error  \\\n",
       "count     569.000000              569.000000    569.000000     569.000000   \n",
       "mean        0.181162                0.062798      0.405172       1.216853   \n",
       "std         0.027414                0.007060      0.277313       0.551648   \n",
       "min         0.106000                0.049960      0.111500       0.360200   \n",
       "25%         0.161900                0.057700      0.232400       0.833900   \n",
       "50%         0.179200                0.061540      0.324200       1.108000   \n",
       "75%         0.195700                0.066120      0.478900       1.474000   \n",
       "max         0.304000                0.097440      2.873000       4.885000   \n",
       "\n",
       "       perimeter error  area error  smoothness error  compactness error  \\\n",
       "count       569.000000  569.000000        569.000000         569.000000   \n",
       "mean          2.866059   40.337079          0.007041           0.025478   \n",
       "std           2.021855   45.491006          0.003003           0.017908   \n",
       "min           0.757000    6.802000          0.001713           0.002252   \n",
       "25%           1.606000   17.850000          0.005169           0.013080   \n",
       "50%           2.287000   24.530000          0.006380           0.020450   \n",
       "75%           3.357000   45.190000          0.008146           0.032450   \n",
       "max          21.980000  542.200000          0.031130           0.135400   \n",
       "\n",
       "       concavity error  concave points error  symmetry error  \\\n",
       "count       569.000000            569.000000      569.000000   \n",
       "mean          0.031894              0.011796        0.020542   \n",
       "std           0.030186              0.006170        0.008266   \n",
       "min           0.000000              0.000000        0.007882   \n",
       "25%           0.015090              0.007638        0.015160   \n",
       "50%           0.025890              0.010930        0.018730   \n",
       "75%           0.042050              0.014710        0.023480   \n",
       "max           0.396000              0.052790        0.078950   \n",
       "\n",
       "       fractal dimension error  worst radius  worst texture  worst perimeter  \\\n",
       "count               569.000000    569.000000     569.000000       569.000000   \n",
       "mean                  0.003795     16.269190      25.677223       107.261213   \n",
       "std                   0.002646      4.833242       6.146258        33.602542   \n",
       "min                   0.000895      7.930000      12.020000        50.410000   \n",
       "25%                   0.002248     13.010000      21.080000        84.110000   \n",
       "50%                   0.003187     14.970000      25.410000        97.660000   \n",
       "75%                   0.004558     18.790000      29.720000       125.400000   \n",
       "max                   0.029840     36.040000      49.540000       251.200000   \n",
       "\n",
       "        worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "count   569.000000        569.000000         569.000000       569.000000   \n",
       "mean    880.583128          0.132369           0.254265         0.272188   \n",
       "std     569.356993          0.022832           0.157336         0.208624   \n",
       "min     185.200000          0.071170           0.027290         0.000000   \n",
       "25%     515.300000          0.116600           0.147200         0.114500   \n",
       "50%     686.500000          0.131300           0.211900         0.226700   \n",
       "75%    1084.000000          0.146000           0.339100         0.382900   \n",
       "max    4254.000000          0.222600           1.058000         1.252000   \n",
       "\n",
       "       worst concave points  worst symmetry  worst fractal dimension  \\\n",
       "count            569.000000      569.000000               569.000000   \n",
       "mean               0.114606        0.290076                 0.083946   \n",
       "std                0.065732        0.061867                 0.018061   \n",
       "min                0.000000        0.156500                 0.055040   \n",
       "25%                0.064930        0.250400                 0.071460   \n",
       "50%                0.099930        0.282200                 0.080040   \n",
       "75%                0.161400        0.317900                 0.092080   \n",
       "max                0.291000        0.663800                 0.207500   \n",
       "\n",
       "           Result  \n",
       "count  569.000000  \n",
       "mean     0.627417  \n",
       "std      0.483918  \n",
       "min      0.000000  \n",
       "25%      0.000000  \n",
       "50%      1.000000  \n",
       "75%      1.000000  \n",
       "max      1.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e0c5454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data['Result'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2471ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83dfbed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.pairplot(data.drop('Result', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e00ff86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6c08ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import (\n",
    "    KFold,\n",
    "    RepeatedKFold,\n",
    "    LeaveOneOut,\n",
    "    LeavePOut,\n",
    "    StratifiedKFold,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73cc04d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.drop('Result', axis=1), data['Result'],\n",
    "                                                   test_size=0.32, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4edb3e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(penalty='l2', solver='liblinear', max_iter=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79666ea2",
   "metadata": {},
   "source": [
    "### K - Fold Cross Validatin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5e52aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = KFold(n_splits=5, shuffle=True, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f64c574b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf =  cross_validate(\n",
    "    logit,\n",
    "    X_train, \n",
    "    y_train,\n",
    "    scoring='accuracy',\n",
    "    return_train_score=True,\n",
    "    cv=k_fold, # k-fold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4cf3ebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96153846, 0.93506494, 0.93506494, 0.96103896, 0.90909091])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf['test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2c2ff6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.01938701, 0.11006522, 0.06463242, 0.01167154, 0.01161361]),\n",
       " 'score_time': array([0.00755358, 0.02376223, 0.01633334, 0.00385833, 0.00368571]),\n",
       " 'test_score': array([0.96153846, 0.93506494, 0.93506494, 0.96103896, 0.90909091]),\n",
       " 'train_score': array([0.94805195, 0.97087379, 0.95469256, 0.95469256, 0.96116505])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1666b8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean train set accuracy:  0.9578951792544025  +-  0.007701156700248209\n",
      "mean test set accuracy:  0.9403596403596403  +-  0.01954470840792648\n"
     ]
    }
   ],
   "source": [
    "print('mean train set accuracy: ', np.mean(clf['train_score']), ' +- ', np.std(clf['train_score']))\n",
    "print('mean test set accuracy: ', np.mean(clf['test_score']), ' +- ', np.std(clf['test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064b6a5c",
   "metadata": {},
   "source": [
    "## Repeated K fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3d17c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We expect K * n performance metrics:  50\n"
     ]
    }
   ],
   "source": [
    "rkf = RepeatedKFold(n_splits=5, n_repeats=10, random_state=0)\n",
    "\n",
    "print('We expect K * n performance metrics: ', 5*10)\n",
    "\n",
    "clf = cross_validate(logit, X_train, y_train, \n",
    "                    scoring='accuracy', return_train_score=True, cv=rkf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a3405c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of metrics obtained:  50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.93589744, 0.94805195, 0.97402597, 0.94805195, 0.93506494,\n",
       "       0.93589744, 0.92207792, 0.93506494, 0.96103896, 0.94805195,\n",
       "       0.91025641, 0.93506494, 0.97402597, 0.94805195, 0.88311688,\n",
       "       0.96153846, 0.90909091, 0.93506494, 0.96103896, 0.93506494,\n",
       "       0.96153846, 0.96103896, 0.94805195, 0.93506494, 0.92207792,\n",
       "       0.92307692, 0.97402597, 0.97402597, 0.90909091, 0.94805195,\n",
       "       0.98717949, 0.92207792, 0.93506494, 0.97402597, 0.90909091,\n",
       "       0.92307692, 0.96103896, 0.94805195, 0.94805195, 0.92207792,\n",
       "       0.92307692, 0.96103896, 0.94805195, 0.94805195, 0.92207792,\n",
       "       0.96153846, 0.90909091, 0.94805195, 0.94805195, 0.98701299])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Number of metrics obtained: ', len(clf['test_score']))\n",
    "\n",
    "clf['test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9513d501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean train set accuracy:  0.9594550918337328  +-  0.007196518449125315\n",
      "mean test set accuracy:  0.9427472527472527  +-  0.022289765023563016\n"
     ]
    }
   ],
   "source": [
    "print('mean train set accuracy: ', np.mean(clf['train_score']), ' +- ', np.std(clf['train_score']))\n",
    "print('mean test set accuracy: ', np.mean(clf['test_score']), ' +- ', np.std(clf['test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6196264f",
   "metadata": {},
   "source": [
    "## Leave One out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4dab1847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We expect as many metrics as data in the train set:  386\n"
     ]
    }
   ],
   "source": [
    "loo = LeaveOneOut()\n",
    "\n",
    "print('We expect as many metrics as data in the train set: ', len(X_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "736cd236",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf =  cross_validate(\n",
    "    logit,\n",
    "    X_train, \n",
    "    y_train,\n",
    "    scoring='accuracy',\n",
    "    return_train_score=True,\n",
    "    cv=loo, # k-fold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c322b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of metrics obtained:  386\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "386"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Number of metrics obtained: ', len(clf['test_score']))\n",
    "\n",
    "len(clf['test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab458a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean train set accuracy:  0.9561402328241707  +-  0.001189724630277267\n",
      "mean test set accuracy:  0.9455958549222798  +-  0.22681343451410146\n"
     ]
    }
   ],
   "source": [
    "print('mean train set accuracy: ', np.mean(clf['train_score']), ' +- ', np.std(clf['train_score']))\n",
    "print('mean test set accuracy: ', np.mean(clf['test_score']), ' +- ', np.std(clf['test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5f449d",
   "metadata": {},
   "source": [
    "## Leave P out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6702f1de",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m lpo \u001b[38;5;241m=\u001b[39m LeavePOut(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m clf \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlpo\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:72\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPass \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m as keyword args. From version 0.25 \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassing these as positional arguments will \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     69\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult in an error\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(args_msg)),\n\u001b[1;32m     70\u001b[0m                   \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[1;32m     71\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mupdate({k: arg \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args)})\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:242\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    240\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    241\u001b[0m                     pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 242\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m zipped_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mscores))\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_train_score:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py:1044\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1044\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1045\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1048\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py:859\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 859\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py:777\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    776\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 777\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:525\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    522\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    524\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m _safe_split(estimator, X, y, train)\n\u001b[0;32m--> 525\u001b[0m X_test, y_test \u001b[38;5;241m=\u001b[39m \u001b[43m_safe_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    528\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y_train \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/metaestimators.py:204\u001b[0m, in \u001b[0;36m_safe_split\u001b[0;34m(estimator, X, y, indices, train_indices)\u001b[0m\n\u001b[1;32m    202\u001b[0m         X_subset \u001b[38;5;241m=\u001b[39m X[np\u001b[38;5;241m.\u001b[39mix_(indices, train_indices)]\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 204\u001b[0m     X_subset \u001b[38;5;241m=\u001b[39m \u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     y_subset \u001b[38;5;241m=\u001b[39m _safe_indexing(y, indices)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/__init__.py:393\u001b[0m, in \u001b[0;36m_safe_indexing\u001b[0;34m(X, indices, axis)\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpecifying the columns using strings is only supported for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas DataFrames\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    390\u001b[0m     )\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_pandas_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _array_indexing(X, indices, indices_dtype, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/__init__.py:195\u001b[0m, in \u001b[0;36m_pandas_indexing\u001b[0;34m(X, key, key_dtype, axis)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# check whether we should index with loc or iloc\u001b[39;00m\n\u001b[1;32m    194\u001b[0m indexer \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39miloc \u001b[38;5;28;01mif\u001b[39;00m key_dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m X\u001b[38;5;241m.\u001b[39mloc\n\u001b[0;32m--> 195\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m indexer[:, key] \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mindexer\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexing.py:895\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    892\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    894\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m--> 895\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexing.py:1492\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1490\u001b[0m \u001b[38;5;66;03m# a list of integers\u001b[39;00m\n\u001b[1;32m   1491\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like_indexer(key):\n\u001b[0;32m-> 1492\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_list_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1494\u001b[0m \u001b[38;5;66;03m# a single integer\u001b[39;00m\n\u001b[1;32m   1495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1496\u001b[0m     key \u001b[38;5;241m=\u001b[39m item_from_zerodim(key)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexing.py:1474\u001b[0m, in \u001b[0;36m_iLocIndexer._get_list_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;124;03mReturn Series values by list or array of integers.\u001b[39;00m\n\u001b[1;32m   1459\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m \u001b[38;5;124;03m`axis` can only be zero.\u001b[39;00m\n\u001b[1;32m   1472\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1473\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_with_is_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1475\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   1476\u001b[0m     \u001b[38;5;66;03m# re-raise with different error message\u001b[39;00m\n\u001b[1;32m   1477\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositional indexers are out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/generic.py:3599\u001b[0m, in \u001b[0;36mNDFrame._take_with_is_copy\u001b[0;34m(self, indices, axis)\u001b[0m\n\u001b[1;32m   3590\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   3591\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_with_is_copy\u001b[39m(\u001b[38;5;28mself\u001b[39m: FrameOrSeries, indices, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FrameOrSeries:\n\u001b[1;32m   3592\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3593\u001b[0m \u001b[38;5;124;03m    Internal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[1;32m   3594\u001b[0m \u001b[38;5;124;03m    attribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3597\u001b[0m \u001b[38;5;124;03m    See the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[1;32m   3598\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3599\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3600\u001b[0m     \u001b[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n\u001b[1;32m   3601\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39m_get_axis(axis)\u001b[38;5;241m.\u001b[39mequals(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis(axis)):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/generic.py:3585\u001b[0m, in \u001b[0;36mNDFrame.take\u001b[0;34m(self, indices, axis, is_copy, **kwargs)\u001b[0m\n\u001b[1;32m   3581\u001b[0m nv\u001b[38;5;241m.\u001b[39mvalidate_take((), kwargs)\n\u001b[1;32m   3583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n\u001b[0;32m-> 3585\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3586\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_block_manager_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m   3587\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3588\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(new_data)\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtake\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/internals/managers.py:1474\u001b[0m, in \u001b[0;36mBlockManager.take\u001b[0;34m(self, indexer, axis, verify, convert)\u001b[0m\n\u001b[1;32m   1471\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndices must be nonzero and less than the axis length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1473\u001b[0m new_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m-> 1474\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m   1476\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/internals/managers.py:1311\u001b[0m, in \u001b[0;36mBlockManager.reindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice)\u001b[0m\n\u001b[1;32m   1307\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slice_take_blocks_ax0(\n\u001b[1;32m   1308\u001b[0m         indexer, fill_value\u001b[38;5;241m=\u001b[39mfill_value, only_slice\u001b[38;5;241m=\u001b[39monly_slice\n\u001b[1;32m   1309\u001b[0m     )\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1311\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1312\u001b[0m         blk\u001b[38;5;241m.\u001b[39mtake_nd(\n\u001b[1;32m   1313\u001b[0m             indexer,\n\u001b[1;32m   1314\u001b[0m             axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   1315\u001b[0m             fill_value\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1316\u001b[0m                 fill_value \u001b[38;5;28;01mif\u001b[39;00m fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mfill_value\n\u001b[1;32m   1317\u001b[0m             ),\n\u001b[1;32m   1318\u001b[0m         )\n\u001b[1;32m   1319\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[1;32m   1320\u001b[0m     ]\n\u001b[1;32m   1322\u001b[0m new_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n\u001b[1;32m   1323\u001b[0m new_axes[axis] \u001b[38;5;241m=\u001b[39m new_axis\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/internals/managers.py:1312\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1307\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slice_take_blocks_ax0(\n\u001b[1;32m   1308\u001b[0m         indexer, fill_value\u001b[38;5;241m=\u001b[39mfill_value, only_slice\u001b[38;5;241m=\u001b[39monly_slice\n\u001b[1;32m   1309\u001b[0m     )\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1311\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m-> 1312\u001b[0m         \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m            \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1318\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1319\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[1;32m   1320\u001b[0m     ]\n\u001b[1;32m   1322\u001b[0m new_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n\u001b[1;32m   1323\u001b[0m new_axes[axis] \u001b[38;5;241m=\u001b[39m new_axis\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/internals/blocks.py:1385\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[0;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[1;32m   1382\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1383\u001b[0m     allow_fill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 1385\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43malgos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[1;32m   1387\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1389\u001b[0m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;66;03m#  this assertion\u001b[39;00m\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m new_mgr_locs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/algorithms.py:1715\u001b[0m, in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, out, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m   1711\u001b[0m     mask_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1712\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1713\u001b[0m     \u001b[38;5;66;03m# check for promotion based on types only (do this first because\u001b[39;00m\n\u001b[1;32m   1714\u001b[0m     \u001b[38;5;66;03m# it's faster than computing a mask)\u001b[39;00m\n\u001b[0;32m-> 1715\u001b[0m     dtype, fill_value \u001b[38;5;241m=\u001b[39m \u001b[43mmaybe_promote\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1716\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;241m!=\u001b[39m arr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mand\u001b[39;00m (out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m dtype):\n\u001b[1;32m   1717\u001b[0m         \u001b[38;5;66;03m# check if promotion is actually required based on indexer\u001b[39;00m\n\u001b[1;32m   1718\u001b[0m         mask \u001b[38;5;241m=\u001b[39m indexer \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/dtypes/cast.py:634\u001b[0m, in \u001b[0;36mmaybe_promote\u001b[0;34m(dtype, fill_value)\u001b[0m\n\u001b[1;32m    631\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 634\u001b[0m     mst \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_scalar_type\u001b[49m(fill_value)\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mst \u001b[38;5;241m>\u001b[39m dtype:\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;66;03m# e.g. mst is np.float64 and dtype is np.float32\u001b[39;00m\n\u001b[1;32m    637\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m mst\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lpo = LeavePOut(p=2)\n",
    "\n",
    "clf = cross_validate(logit, X_train, y_train, scoring='accuracy', cv=lpo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a3dbd8",
   "metadata": {},
   "source": [
    "## Grid Search With Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1627eea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cb6ed8",
   "metadata": {},
   "source": [
    "## 1. K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "00ddc066",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(max_iter=1000, solver='liblinear')\n",
    "\n",
    "param_grid =dict(penalty=['l1','l2'],\n",
    "                C=[0.1,1,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b37eabf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "01cfc3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(logit, param_grid, scoring='accuracy',\n",
    "                  cv=kf, refit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "26ecaf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "499a3362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'penalty': 'l1'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "94866342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'C': 0.1, 'penalty': 'l1'}</td>\n",
       "      <td>0.904063</td>\n",
       "      <td>0.021349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'C': 0.1, 'penalty': 'l2'}</td>\n",
       "      <td>0.927439</td>\n",
       "      <td>0.026792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'C': 1, 'penalty': 'l1'}</td>\n",
       "      <td>0.945521</td>\n",
       "      <td>0.017340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'C': 1, 'penalty': 'l2'}</td>\n",
       "      <td>0.940360</td>\n",
       "      <td>0.019545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'C': 10, 'penalty': 'l1'}</td>\n",
       "      <td>0.955911</td>\n",
       "      <td>0.017686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'C': 10, 'penalty': 'l2'}</td>\n",
       "      <td>0.950716</td>\n",
       "      <td>0.017320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        params  mean_test_score  std_test_score\n",
       "0  {'C': 0.1, 'penalty': 'l1'}         0.904063        0.021349\n",
       "1  {'C': 0.1, 'penalty': 'l2'}         0.927439        0.026792\n",
       "2    {'C': 1, 'penalty': 'l1'}         0.945521        0.017340\n",
       "3    {'C': 1, 'penalty': 'l2'}         0.940360        0.019545\n",
       "4   {'C': 10, 'penalty': 'l1'}         0.955911        0.017686\n",
       "5   {'C': 10, 'penalty': 'l2'}         0.950716        0.017320"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(search.cv_results_)[['params','mean_test_score', 'std_test_score']]\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "39209ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values(by='mean_test_score', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4c674b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<AxesSubplot:>], dtype=object)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results.reset_index(drop=True, inplace=True)\n",
    "results['mean_test_score'].plot(yerr=[results['std_test_score'], results['std_test_score']], subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e8eb6d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.9740932642487047\n",
      "Test Accuracy:  0.9726775956284153\n"
     ]
    }
   ],
   "source": [
    "train_preds = search.predict(X_train)\n",
    "test_preds = search.predict(X_test)\n",
    "\n",
    "print('Train Accuracy: ', accuracy_score(y_train, train_preds))\n",
    "print('Test Accuracy: ', accuracy_score(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0ed3bd",
   "metadata": {},
   "source": [
    "## 2.Repeated K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "28addfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rkf = RepeatedKFold(n_splits=5, n_repeats=10, random_state=4)\n",
    "\n",
    "clf = GridSearchCV(logit,\n",
    "                  param_grid, scoring='accuracy',\n",
    "                  cv=rkf, refit=True)\n",
    "search = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "55c5e3a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'penalty': 'l1'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6d13b5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(search.cv_results_)\n",
    "results = results[['params','mean_test_score','std_test_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9dc636e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'C': 10, 'penalty': 'l1'}</td>\n",
       "      <td>0.954402</td>\n",
       "      <td>0.021858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'C': 10, 'penalty': 'l2'}</td>\n",
       "      <td>0.949474</td>\n",
       "      <td>0.022312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'C': 1, 'penalty': 'l1'}</td>\n",
       "      <td>0.945594</td>\n",
       "      <td>0.024833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'C': 1, 'penalty': 'l2'}</td>\n",
       "      <td>0.943766</td>\n",
       "      <td>0.025355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'C': 0.1, 'penalty': 'l2'}</td>\n",
       "      <td>0.930330</td>\n",
       "      <td>0.026518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'C': 0.1, 'penalty': 'l1'}</td>\n",
       "      <td>0.916337</td>\n",
       "      <td>0.028513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        params  mean_test_score  std_test_score\n",
       "4   {'C': 10, 'penalty': 'l1'}         0.954402        0.021858\n",
       "5   {'C': 10, 'penalty': 'l2'}         0.949474        0.022312\n",
       "2    {'C': 1, 'penalty': 'l1'}         0.945594        0.024833\n",
       "3    {'C': 1, 'penalty': 'l2'}         0.943766        0.025355\n",
       "1  {'C': 0.1, 'penalty': 'l2'}         0.930330        0.026518\n",
       "0  {'C': 0.1, 'penalty': 'l1'}         0.916337        0.028513"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values(by='mean_test_score', ascending=False, inplace=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "548d9846",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0ba4c1ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'C': 10, 'penalty': 'l1'}</td>\n",
       "      <td>0.954402</td>\n",
       "      <td>0.021858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'C': 10, 'penalty': 'l2'}</td>\n",
       "      <td>0.949474</td>\n",
       "      <td>0.022312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'C': 1, 'penalty': 'l1'}</td>\n",
       "      <td>0.945594</td>\n",
       "      <td>0.024833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'C': 1, 'penalty': 'l2'}</td>\n",
       "      <td>0.943766</td>\n",
       "      <td>0.025355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'C': 0.1, 'penalty': 'l2'}</td>\n",
       "      <td>0.930330</td>\n",
       "      <td>0.026518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'C': 0.1, 'penalty': 'l1'}</td>\n",
       "      <td>0.916337</td>\n",
       "      <td>0.028513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        params  mean_test_score  std_test_score\n",
       "0   {'C': 10, 'penalty': 'l1'}         0.954402        0.021858\n",
       "1   {'C': 10, 'penalty': 'l2'}         0.949474        0.022312\n",
       "2    {'C': 1, 'penalty': 'l1'}         0.945594        0.024833\n",
       "3    {'C': 1, 'penalty': 'l2'}         0.943766        0.025355\n",
       "4  {'C': 0.1, 'penalty': 'l2'}         0.930330        0.026518\n",
       "5  {'C': 0.1, 'penalty': 'l1'}         0.916337        0.028513"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c54fb351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Hyperparameter space')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEJCAYAAACOr7BbAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAlb0lEQVR4nO3deXhV5bn+8e+TiUAGhiTMSBiighaniCIgQ61HW4eibdUerVorepyPx6L+7E9bO9g6nUodjtbpYK1ocWy1KmUQQVCCCIKMMsiogDKDQPKcP9ZK2IRF2GB2dpJ9f65rX1nzftYWc+dd79rvMndHRESkurRkFyAiIvWTAkJERCIpIEREJJICQkREIikgREQkkgJCREQiJTQgzOw0M5tnZgvN7JaI9Z3NbIyZzTSz8WbWMWbd3WY228zmmNlwM7NE1ioiIntKWECYWTrwEHA60BO4wMx6VtvsXmCEu/cC7gTuCvc9CegL9AKOBI4HBiSqVhER2VtGAo/dG1jo7osAzGwkcDbwScw2PYEbw+lxwCvhtAPZQBZgQCbweU1vVlhY6MXFxbVUuohIapg2bdpady+KWpfIgOgALIuZXw6cUG2bGcA5wAPAECDPzArcfbKZjQNWEQTEg+4+p6Y3Ky4upqysrNaKFxFJBWa2dF/rkt1JfRMwwMymE1xCWgGUm1l3oAfQkSBoBptZ/+o7m9lQMyszs7I1a9bUZd0iIo1eIgNiBdApZr5juKyKu69093Pc/RjgtnDZeoLWxBR33+zum4F/An2qv4G7P+bupe5eWlQU2UISEZGDlMiAmAqUmFkXM8sCzgdei93AzArNrLKGW4Enw+nPCFoWGWaWSdC6qPESk4iI1K6EBYS77wKuAd4i+OX+grvPNrM7zeyscLOBwDwzmw+0AX4bLh8FfAp8TNBPMcPd/56oWkVEZG/WWIb7Li0tdXVSi4gcGDOb5u6lUeuS3UktIiL1lAJCREQiKSBERCSSAgI479HJnPfo5GSXISJSryggREQkkgJCREQiKSBERCSSAkJERCIpIEREJJICQkREIikgREQkkgJCREQiKSBERCSSAiJF6dvjIrI/CggREYmkgBARkUgKCEkJuqQmcuAUECIiEkkBISIikRQQIo2ULqvJN6WAEBGRSAoIERGJpIAQEZFICggREYmkgBARkUgKCBERiaSAEBGRSAoIERGJpIAQEZFICggREYmkgBARkUgKCBERiaSAEBGRSAoIERGJpIAQEZFICggREYmU0IAws9PMbJ6ZLTSzWyLWdzazMWY208zGm1nHmHWHmNnbZjbHzD4xs+JE1ioiIntKWECYWTrwEHA60BO4wMx6VtvsXmCEu/cC7gTuilk3ArjH3XsAvYEvElWriIjsLZEtiN7AQndf5O47gJHA2dW26QmMDafHVa4PgyTD3UcDuPtmd9+awFpFRKSaRAZEB2BZzPzycFmsGcA54fQQIM/MCoBDgfVm9pKZTTeze8IWiYiI1JFkd1LfBAwws+nAAGAFUA5kAP3D9ccDXYFLqu9sZkPNrMzMytasWVNnRYuIpIJEBsQKoFPMfMdwWRV3X+nu57j7McBt4bL1BK2Nj8LLU7uAV4Bjq7+Buz/m7qXuXlpUVJSYsxARSVGJDIipQImZdTGzLOB84LXYDcys0Mwqa7gVeDJm3xZmVvlbfzDwSQJrFRGRahIWEOFf/tcAbwFzgBfcfbaZ3WlmZ4WbDQTmmdl8oA3w23DfcoLLS2PM7GPAgD8nqlYREdlbRiIP7u5vAG9UW3Z7zPQoYNQ+9h0N9EpkfSIism/J7qQWEZF6SgEhIiKRFBAi0mic9+hkznt0crLLaDQUECIiEkkBISIikRQQIiISKeUDYmd5BfM/38Syr7by+sxVLFqzmfIKT3ZZIiJJl9DvQTQE6zbvYPvOCr7aupOr//ohANmZaRzWJo8e7fI5vG0eh7fLp0fbfJo3y0xytSIidSflA6Jt82x6dWxORYVzx1lHMGfVRuas2sTc1Rt5a/ZqRk7dPSBt++bZQVi0y+PwtsHP4oIcMtJTviEmIo1QygdEpbQ048gOzTmyQ/OqZe7OF5u+Zs6qjcxdvYm5YXhMmL+GXeFlqCYZaRzaJm93S6NdHj3a5tMyJytZpyIiUisUEDUwM9rkZ9MmP5uBh7WuWv71rnI+/WJLGBxBeIyb9wV/m7a8aps2+U3CS1RhaLTLp0thDplqbYhIA6GAOAhNMtLp2T6fnu3z91i+ZtPXzF29MQiOVZuYs3oTkxYuYmd50NrISk+je+tcelS7TFWQ2yQZpyEiUiMFRC0qymtCUV4R/Ut2P5tix64KFq3dHAZGcInq3QVrePHD5Xvsd3jbvD2Co1tRLlkZam2ISPIoIBIsKyONw9sGl5q+H/PE1XWbv2bu6k1V/RtzVm3k6UlL2FFeAUBmutGtKLfqTqoe7fI5vF0eRblNMLNknY6IpBAFRJIU5Dahb/cm9O1eWLVsV3kFi9duYU5lcKzayJRF63h5+u4H8RXkZO15+227PLq3zqVJhh7ZLSK1SwFRj2Skp1HSJo+SNnmcdVT7quVfbdkR3EVV2b+xehPPTFnK17uC1kZ6mtGtKKeqU/zw8E6qNvlqbYjIwVNANAAtc7Lo062APt0KqpaVVziL124J7qJaFbQ4ypZ8xasfrdy9X7PM3YERftmvpE0u2ZlqbYjI/ikgGqj0NKN761y6t87ljJjn7m3YtpO5ld/bCDvFR36wjG07ywFIM+halMuXW3bQNDONF6ctp7gwh+KCZrTKyVKLQ0SqKCAameZNMzmhawEndN2ztfHZl1vDL/ptZM7qTSz7citfbqngv/42o2q7vOwMigtyqgIjmA5+KjxEUo8CIgWkpxldCnPoUpjD6d9qBwQPVqlw5/fn9mLpui0sXrs1/LmFGcvW8/rMlcSOWZjXJIPiwhw6FzSjS2EOnQty6FLYjM4FORQoPEQapf0GhJm9BDwB/NPdKxJfktSVNAtupe1WlLvXuh27Klj+1VaWrtvK4rVbgvBYt5WPV2zgn7NW7zHibV6TDDqHYdGlYM8QKcxVeIg0VPG0IB4GLgWGm9nfgKfcfV5iy5Jky8pIo2tRLl2LchlUbd2OXRWsWL+NJWu3sGTdlvDnVmat2MCb1cIjt0kGnQuaVbtslUOxwkOk3ttvQLj7v4B/mVlz4IJwehnwZ+Av7r4zwTVKPZOVkVZ1yaq6neUVLP9qW1VwVLZAZtcUHmFfR3DZKmiB6AuBIskXVx+EmRUAFwIXAdOBZ4F+wMXAwEQVJw1PZnpMeBy257qd5RWsiAmPJeu2smTdFj5ZFQytvismPHKy0vcIjMpWR3FBM4ryFB4idSGePoiXCf5XfwY4091XhaueN7OyRBYnjUtmelrwi34f4bFy/bawv2N3v0dUeDSrCo+9+z0UHiK1J54WxHB3Hxe1wt1La7keSVGZ6Wl0Lgg6tqvbVR72eazbuke/x9xVm3h79ueR4VFcrd9jx64KMtMVHCIHIp6A6Glm0919PYCZtQQucPeHE1qZSCgjJjwGHFq0x7pd5RWsXL+dxeuCFseStcFlq3mfb+Jfcz6vGmodwAxO+N2/aNE0i+ZNM8lvmknzmFeLZrun86st03M8JBXFExCXu/tDlTPu/pWZXU5wd5NIUmWkp3FIQTMOKWgG7B0eqzZsZ/HaLdz+6iy+3lVB/5JCNmzbyYZtO1mxfhufrNzAhm072bKjvMb3aZaVvkd4tKgWLs2bVZuPeemRtNJQxRMQ6WZm7u4AZpYO6HmaUu9lpKfRqVUzOrVqRpv8bADu/sFRkdvuLK9gYxgc68OflfMbtu5eVvn67MutwbZbd1YNY7IvuU0yYlolGTEtln20ZGKCKD1Nl8WkZuc9OhmA56/oU+vHjicg3iTokH40nL8iXCbSaGSmp1GQ2+Sgnu63Y1fFHuGxYduOqmDZsG1XGDo7qgJn8dotVdtu31nzd0/zmmTsdblrj5bMPlouedmZB/tRiFSJJyBuJgiF/wjnRwOPJ6wikQYmKyMtfJrggYfL9p3lu1sqlS2YrXvOx7ZsFnyxuWr5jl37Dhez4JvyTTLSeOnD5Zx1VHtd6pIDFs8X5SqAR8KXiNSi7Mx0sjPTaR1eAjsQ23eW7xUmVa+tO3i+bBmbtu/ixhdmMHzMAq4e1J3vH9NBHe4St3i+B1EC3AX0BKr+Fbt71wTWJSL7kZ2ZTtvm6bRtHh0u7y/+Enfnsv5dGT5mAT8fNZPhYxdw9cDunHNsRz3zXPYrnn8hTxG0HnYBg4ARwF8SWZSI1A4z49+OaMs/ru3HExeX0rJZFre89DGD7h3Ps+8v5etdNXewS2qLJyCauvsYwNx9qbv/EvheYssSkdpkZny7RxtevbovT116PEV5Tbjt5VkMvGc8IyYvYft+7sSS1BRPQHxtZmnAAjO7xsyGAHuPDy0i9Z6ZMeiw1rx81Uk8c1lvOrRoyu2vzmbAPeN4atJiBYXsIZ6AuB5oBlwHHEcwaN/F8RzczE4zs3lmttDMbolY39nMxpjZTDMbb2Ydq63PN7PlZvZgPO8nIvExM/qXFPG3K/vw15+dQHFBDr/6+yf0v3scj7+7iG37+eKgpIYaAyL8Utx57r7Z3Ze7+6Xufq67T9nfgcN9HwJOJ+jgvsDMelbb7F5ghLv3Au4k6AyP9WtgQpznIiIHyMw4qXshz1/Rh5FDT6SkdS6/eX0O/e8ey6PvfMqWr3clu0RJohoDwt3LCYb1Phi9gYXuvsjddwAjgbOrbdMTGBtOj4tdb2bHAW2Atw/y/UXkAJzYtYC/Xn4if7uyDz3a5XPXP+fS/+5xPDx+IZsVFCkpnktM083sNTO7yMzOqXzFsV8HYFnM/PJwWawZQOWxhgB5ZlYQ9nncB9xU0xuY2VAzKzOzsjVr1sRRkojsz/HFrXjmshN48T9OolfH5tz95jz6/WEsD45dwMbtej5YKoknILKBdcBg4MzwdUYtvf9NwAAzmw4MAFYA5cBVwBvuvrymnd39MXcvdffSoqKimjYVkQN0XOeWPH1pb169ui/HHdKSe9+eT7/fj+WP/5rPhm0KilQQzzepLz3IY68AOsXMdwyXxR57JWELwsxygXPdfb2Z9QH6m9lVBHdMZZnZZnffq6NbRBLrqE4teOKS45m1YgPDxyzgj/9awBPvLubSvsX8tF8XWjTT2J2NVTzfpH4K8OrL3f2n+9l1KlBiZl0IguF84MfVjl0IfBkO53Er8GR47H+P2eYSoFThIJJcR3ZozmM/KWX2yg08OHYhw8cu5ImJi7n4pGJ+1r8rrXIUFI1NPIP1/SNmOpugr2Dl/nZy911mdg3wFpAOPOnus83sTqDM3V8jeJ71XWbmBHcrXX2A9YtIHTuifXMeufA45q3exJ/GLuCRdz7l6feWcFGfzlzevyuFBzEirtRP8VxiejF23syeAybGc3B3fwN4o9qy22OmRwGj9nOMp4Gn43k/Eak7h7XN48EfH8v1n2/iwXEL+fOERfzve0u48ITODB3QldZ5Bz4AodQvBzNaVwnQurYLEZGGqaRNHg+cfwyjbxzAd49sx5OTFtP/D+P41d9n8/nG7ckuT76BePogNrFnH8RqgmdEiIhU6VaUy/3nHc113y7hoXELGTF5Kc++/xkXHN+JKwd2o13zpskuUQ5QPJeY8uqiEBFpHIoLc7jnh0dx7eASHh6/kGff/4znPljGD0s7ctWg7nRooaBoKPZ7icnMhphZ85j5Fmb2/YRWJSIN3iEFzfj9ub0Y//OB/KC0Iy+ULWPgPeO49aWZLPtya7LLkzjE0wdxh7tvqJxx9/XAHQmrSEQalY4tm/G7Id/inZ8P4oLeh/DitBUMunc8w0bNYOm6LckuT2oQT0BEbRPP7bEiIlXat2jKnWcfyYRhg7jwxM68+tFKBt/3Dje+8BGL1mxOdnkSIZ6AKDOz+82sW/i6H5iW6MJEpHFq2zybX551BO8OG8SlJxXzxserOOX+d7hh5HQWfrEp2eVJjHgC4lpgB/A8wYis29EX2kTkG2qdn80vzujJxJsHc3n/rrw1+3O+898TuOavHzJvtYKiPojnLqYtgIa5EJGEKMxtwq3f7cHQk7vy+MTFjHhvCf+YuYrvfqst1w4uoUe7/GSXmLLiuYtptJm1iJlvaWZvJbQqEUk5BblNuPm0w5l482CuHdydd+ev5fQH3mXoiDJmrdiw/wNIrYuns7kwvHMJAHf/ysz0TWoRSYiWOVn816mH8bN+XXly0mKenLSYtz/5nFN6tOa6b5fQq2OLZJeYMuIJiAozO8TdP4PgOdJEjO7akD1/RZ9klyAi1TRvlsl/fudQLuvfhf+dtITHJy7mrAcnMeiwIq77dgnHHNIy2SU2evEExG3ARDN7BzCgP3BFQqsSEQnlZ2dy7bdLuKRvMSMmL+Xxdxcx5OH36F9SyA2nlHBc51bJLrHRiqeT+k0zOxY4MVx0A6ALgiJSp/KyM7l6UHcuOamYZ6Ys5c8TFnHuI5Pp272A6waXcELXgmSX2OjENZqru68FXge2AX8geL60iEidy2mSwZUDuvHuzYP4xfd6MG/1Zs57bArnPTqZDdt24t6oroAnVTx3MZ1oZsOBpcCrBA/2OTzRhYmI1KRZVgY/69+ViTcP4vYzerJ47Rbmrt7EnFWbmLJoXbLLaxT2GRBm9jszWwD8FpgJHAOscff/dfev6qpAEZGaZGem89N+XZgwbBCdC5qxfVc55z82hX9/fArTlupX1TdRUwviZ8DnwCPAM+6+jkZ295KINB7Zmem0zc/m6I4t+MX3ejB31SbOfeQ9Ln3qAz5erm7Tg1FTQLQDfgOcCXxqZs8ATc1MA/WJSL2Vlmb8rH9XJgwbxLDTDuPDz9Zz5oMTueKZMuau3pjs8hqUff6yd/dy4E3gTTNrApwBNAVWmNkYd/9xHdUoInLAcppkcNXA7lx4YmeenLiYJ95dzNufvMsZvdpzwykldCvKTXaJ9V5crQF3/xp4EXjRzPKB7yeyKBGR2pKfnckNpxzKJScV89iERTw1aQmvz1zJkGM6cv23SzikoFmyS6y34rrNNZa7b3T3EYkoRkQkUVo0y2LYaYfz7s2D+GnfLvxj5koG3zeeW1/6mJXrtyW7vHrpgANCRKQhK8xtwi/O6MmEYYP48QmHMGraMgbeM55fvjabLzZuT3Z59Yo6nFOUxp+SVNcmP5s7zz6SoSd35cGxC3lmylJGTv2Mi/sUc8WAbrTKyUp2iUkXV0CY2UlAcez2uswkIo1Bx5bN+P25vbhyQDeGj1nAY+8u4i9TlnJp3y5c3r8rzZtlJrvEpInnm9TPAPcC/YDjw1dpgusSEalTxYU53H/e0Yz+z5MZeHhrHhy3kH53j2X4mAVs2r4z2eUlRTwtiFKgp2uAExFJAd1b5/HQj4/l6oEbuX/0fO4fPZ+nJi3mygHd+EmfYppmpSe7xDoTTyf1LKBtogsREalPerbP5/GLS3nl6r58q2ML7vrnXPrfPY6nJi1m+87yZJdXJ+J6ohzwiZl9AHxdudDdz0pYVSIi9cTRnVow4qe9mbrkS+57ex6/+vsnPDZhEdcM7s4Pj+tEVkbjvRk0noD4ZaKLEBGp744vbsXIoX14b+Fa7n17Hre9PItHxn/K9d8uYcgxHchIb3xBEc8Dg96pi0JERBqCk7oX8mK3AsbPX8N9b8/j56NmBkFxSgln9mpPWpolu8RaE+/zIKaa2WYz22Fm5WamEa9EJGWZGYMOa83fr+nHoxcdR2Z6GteP/IjTHpjAm7NWNZqHFsXTJnoQuABYQDBY38+AhxJZlIhIQ2Bm/NsRbfnn9f0ZfsEx7KpwrvzLh5zxp4mMnft5gw+KeB85uhBId/dyd38KOC2xZYmINBxpacZZR7Xn7RtO5r4fHsXG7Tv56dNlDHn4PSYuWNtggyKeTuqtZpYFfGRmdwOr0BhOIiJ7yUhP49zjOnLW0e0ZNW05w8cs4MIn3ueELq34r1MPo3eXVsku8YDE84v+onC7a4AtQCfg3HgObmanmdk8M1toZrdErO9sZmPMbKaZjTezjuHyo81sspnNDtedF/8piYgkV2Z6Ghf0PoRxNw3kl2f2ZNHaLfzo0clc9MT7TP+s4TwGdb8B4e5LAQPaufuv3P3G8JJTjcwsnaCv4nSgJ3CBmfWsttm9wAh37wXcCdwVLt8K/MTdjyC4nPVHM2sR5zmJiNQL2ZnpXNK3CxN+Poj/993Dmb1yI0Mefo/Lnp7KrBX1/zGo8dzFdCbwEcHT5Sr/un8tjmP3Bha6+yJ33wGMBM6utk1PYGw4Pa5yvbvPd/cF4fRK4AugKI73FIn0/BV9NIKtJE3TrHSGntyNCcMGcdOphzJ1yZec8aeJXPXsNBZ8vinZ5e1TPJeYfknwy349gLt/BHSJY78OwLKY+eXhslgzgHPC6SFAnpkVxG5gZr2BLODTON5TRKTeym2SwTWDS3j35sFcN7g778xbw6l/nMANI6ezeO2WZJe3l3gCYqe7V28L1VaX/E3AADObDgwAVgBVg5yYWTvgGeBSd6+ovrOZDTWzMjMrW7NmTS2VJCKSWM2bZnLjqYfx7s2DGXpyV96cvZpT7n+HYaNmsOzLrckur0o8ATHbzH4MpJtZiZn9CXgvjv1WEHRoV+oYLqvi7ivd/Rx3Pwa4LVy2HiB89vXrwG3uPiXqDdz9MXcvdffSoiJdgRKRhqVVTha3nt6DCcMG8ZM+nXllevAY1F+88jGrNyT/6XbxBMS1wBEEA/U9B2wEbohjv6lAiZl1CW+TPR/Yo+/CzArNrLKGW4Enw+VZwMsEHdij4ngvEZEGq3VeNneceQTvDBvIj0o7MfKDZZx8zzju/PsnrNn09f4PkCDx3MW01d1vc/fjw7/Wb3P3/Uabu+8iuDX2LWAO8IK7zzazO82sciTYgcA8M5sPtAF+Gy7/EXAycImZfRS+jj7gsxMRaUDaNW/Kb4d8i3E3DeTso9rz9HuLOfnucfz+n3P5asuOOq/H9vUNv/3dqVTfhvsuLS31srKyZJchIkl03qOTgcbzzPVFazbzwJgFvDZjJTlZGfy0Xxcu69eF5k13Pwb1m56zmU1z98inhNb0Teo+BHchPQe8T/BdCBERqSNdi3J54PxjuGpgd/74r/kMH7OApyct5ooB3bjkpGJymsQzGMbBq+nobYHvEAzU92OCDuPn3H12QisSEZE9HNY2j0cuPI5ZKzbw36Pnc89b83hi4mL+Y0A3Kio8YUOM77MPIhyY7013vxg4EVgIjDezaxJSiYiI1OjIDs154pLjeemqkziifT6/fWMOHy1fz+qN2xMyIGCNndRm1sTMzgH+AlwNDCe4u0hERJLk2ENa8sxlJzBy6IlkZ6SzfutOzGq/FbHPS0xmNgI4EngD+JW7z6r1dxcRkYN2YtcCerTLoyJBo4nX1AdxIcHordcD18WkkwHu7vmJKUlEROJlZqQn6BaifQaEu+uZDyIiKUwhICIikRQQIiISSQEhIiKRFBAiIhJJASEiIpEUECIiEkkBISIikRQQIiISSQEhIiKRFBAiIhJJASEiIpEUECIiEkkBISIikRQQIiISSQEhIiKRFBAiIhJJASEiIpEUECIiEkkBISIikRQQIiISSQEhIiKRFBAiIhJJASEiIpEUECIiEikj2QWIiNSW56/ok+wSGhW1IEREJJICQkREIikgREQkkgJCREQiJTQgzOw0M5tnZgvN7JaI9Z3NbIyZzTSz8WbWMWbdxWa2IHxdnMg6RURkbwkLCDNLBx4CTgd6AheYWc9qm90LjHD3XsCdwF3hvq2AO4ATgN7AHWbWMlG1iojI3hLZgugNLHT3Re6+AxgJnF1tm57A2HB6XMz6fwNGu/uX7v4VMBo4LYG1iohINYkMiA7Aspj55eGyWDOAc8LpIUCemRXEua+IiCRQsjupbwIGmNl0YACwAiiPd2czG2pmZWZWtmbNmkTVKCKSkhIZECuATjHzHcNlVdx9pbuf4+7HALeFy9bHs2+47WPuXurupUVFRbVcvohIaktkQEwFSsysi5llAecDr8VuYGaFZlZZw63Ak+H0W8CpZtYy7Jw+NVwmIiJ1JGEB4e67gGsIfrHPAV5w99lmdqeZnRVuNhCYZ2bzgTbAb8N9vwR+TRAyU4E7w2UiIlJHEjpYn7u/AbxRbdntMdOjgFH72PdJdrcoRESkjiW7k1pEROopBYSIiERSQIiISCQFhIiIRFJAiIhIJAWEiIhEUkCIiEgkBYSIiERSQIiISCQFhIiIRFJAiIhIJAWEiIhEUkCIiEgkBYSIiERSQIiISCQFhIiIRFJAiIhIJAWEiIhEUkCIiEgkBYSIiERSQIiISCQFhIiIRMpIdgEiInLwnr+iT8KOrRaEiIhEUkCIiEgkBYSIiERSQIiISCQFhIiIRFJAiIhIJAWEiIhEUkCIiEgkBYSIiEQyd092DbXCzNYAS7/BIQqBtbVUTkORauecaucLOudU8U3OubO7F0WtaDQB8U2ZWZm7lya7jrqUauecaucLOudUkahz1iUmERGJpIAQEZFICojdHkt2AUmQauecaucLOudUkZBzVh+EiIhEUgtCREQipXxAmNlpZjbPzBaa2S3JrifRzOxJM/vCzGYlu5a6YmadzGycmX1iZrPN7Ppk15RoZpZtZh+Y2YzwnH+V7Jrqgpmlm9l0M/tHsmupK2a2xMw+NrOPzKysVo+dypeYzCwdmA98B1gOTAUucPdPklpYApnZycBmYIS7H5nseuqCmbUD2rn7h2aWB0wDvt/I/zsbkOPum80sE5gIXO/uU5JcWkKZ2Y1AKZDv7mcku566YGZLgFJ3r/XvfqR6C6I3sNDdF7n7DmAkcHaSa0ood58AfJnsOuqSu69y9w/D6U3AHKBDcqtKLA9sDmczw1ej/mvQzDoC3wMeT3YtjUWqB0QHYFnM/HIa+S+OVGdmxcAxwPtJLiXhwsstHwFfAKPdvbGf8x+BYUBFkuuoaw68bWbTzGxobR441QNCUoiZ5QIvAje4+8Zk15No7l7u7kcDHYHeZtZoLyma2RnAF+4+Ldm1JEE/dz8WOB24OryMXCtSPSBWAJ1i5juGy6SRCa/Dvwg86+4vJbueuuTu64FxwGlJLiWR+gJnhdfjRwKDzewvyS2pbrj7ivDnF8DLBJfOa0WqB8RUoMTMuphZFnA+8FqSa5JaFnbYPgHMcff7k11PXTCzIjNrEU43JbgRY25Si0ogd7/V3Tu6ezHB/8dj3f3CJJeVcGaWE954gZnlAKcCtXaHYkoHhLvvAq4B3iLouHzB3Wcnt6rEMrPngMnAYWa23MwuS3ZNdaAvcBHBX5Ufha/vJruoBGsHjDOzmQR/CI1295S59TOFtAEmmtkM4APgdXd/s7YOntK3uYqIyL6ldAtCRET2TQEhIiKRFBAiIhJJASEiIpEUECIiEkkBIUlnZpurzV9iZg8mq55kM7MbzKxZsusQUUBIyjGzjFo4Rnpt1LIPNwAHFBAJrkdSlAJC6i0zyzOzxeEwGZhZfuW8mY03swfCL73NMrPe4TY54TMvPgifC3B2uPwSM3vNzMYCY8xsoJlNMLPXw+eB/I+ZpYXbPmJmZdWfoxCOu/8HM/sQ+KGZXW5mU8NnLrxY+Ve/mT0dHmOKmS0K3+tJM5tjZk/HHO9UM5tsZh+a2d/MLNfMrgPaE3zJbdy+touqp9pn98Pwc5lhZhNiPoNXw89ugZndEbP9K+Fgb7NjB3yz4HkpH4bHGVPTZyyNkLvrpVdSX0A58FHM6zPgwXDdUwTPbgAYCtwXTo8H/hxOnwzMCqd/B1wYTrcgeN5HDnAJwWi9rcJ1A4HtQFcgHRgN/CBcV7lNevg+vcL5JcCwmLoLYqZ/A1wbTj9NMB6QEQwfvxH4FsEfZNOAo4FCYALBMxsAbgZuj3mfwnB6f9sN28dn+jHQofJzCH9eAqwCCoCmBEMylFY758rlBUARwWjHXaptE/kZJ/vfkV61//rGTW2RWrDNg1FHgeAvXYKHvkAwtv8w4BXgUuDymP2eg+AZF2HrogXBWDRnmdlN4TbZwCHh9Gh3j30Wxgfuvih8z+eAfsAo4EfhX9EZBENW9ARmhvs8H7P/kWb2G4JfkrkEQ7ZU+ru7u5l9DHzu7h+H7zMbKCYYGLInMCkYKoosgiFQqjtxP9s9H7EPwCTgaTN7AYgdnHC0u68La3kpPOcy4DozGxJu0wkoIQiICe6+GCDms9vXZzxnH7VIA6WAkHrN3SeZWbGZDQTS3T12ILLq48Q4wV/t57r7vNgVZnYCsCVi+z3mzawLcBNwvLt/FV4Syo7ZJvYYTxO0bmaEoTYwZt3X4c+KmOnK+QyCVtNod7+Amtl+tqt+TsGJuF8ZnvP3gGlmdlzlquqbhp/tKUAfd99qZuPZ85yjatrrM5bGR30Q0hCMAP5KcLkp1nkAZtYP2ODuGwj+ir/Wwj+3zeyYGo7b24KRfNPCY00E8gl+6W4wszYEY+zvSx6wKuwj+fcDPKcpQF8z6x7WmWNmh4brNoXH3t92+2Rm3dz9fXe/HVjD7mHtv2NmrSwY4fX7BC2N5sBXYTgcTtBqqXzvk8PQxMxahcsP5DOWBkwBIQ3Bs0BLwktKMbab2XTgf4DKUWl/TfB4zZnh5Zxf13DcqcCDBJdGFgMvu/sMYDrB0Nh/JfgFui//n+DJdJM4wKG03X0NQZ/AcxaMuDoZODxc/RjwppmN2892NbnHggfZzwLeA2aEyz8geC7GTOBFdy8D3gQyzGwO8HuCYKiscSjwkgWjhVZezjqQz1gaMI3mKvWemf0AONvdL4pZNh64KfwFdzDHHBjunxIPtofdfTvufk2ya5GGQX0QUq+Z2Z8ILvM09uc3iNQ7akGIiEgk9UGIiEgkBYSIiERSQIiISCQFhIiIRFJAiIhIJAWEiIhE+j8wy8T9bbqoFwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results['mean_test_score'].plot(yerr=[results['std_test_score'], \n",
    "                                      results['std_test_score']],\n",
    "                               subplots=True)\n",
    "plt.ylabel('Mean Accuracy')\n",
    "\n",
    "plt.xlabel('Hyperparameter space')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4a949f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.9740932642487047\n",
      "Test Accuracy:  0.9726775956284153\n"
     ]
    }
   ],
   "source": [
    "# let's get the predictions\n",
    "train_preds = search.predict(X_train)\n",
    "test_preds = search.predict(X_test)\n",
    "\n",
    "print('Train Accuracy: ', accuracy_score(y_train, train_preds))\n",
    "print('Test Accuracy: ', accuracy_score(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635ef2c0",
   "metadata": {},
   "source": [
    "## 3.Leave One Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6c78937b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loo = LeaveOneOut()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "57671668",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf  = GridSearchCV(logit, param_grid,\n",
    "                   scoring='accuracy',\n",
    "                   cv=loo,\n",
    "                   refit=True)\n",
    "search= clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c999b1d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Hyperparameter space')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = pd.DataFrame(search.cv_results_)\n",
    "results = results[['params','mean_test_score','std_test_score']]\n",
    "results.sort_values(by='mean_test_score', \n",
    "                   ascending=False, inplace=True)\n",
    "results.reset_index(drop=True, inplace=True)\n",
    "results['mean_test_score'].plot(yerr=[results['std_test_score'],\n",
    "                                     results['std_test_score']],\n",
    "                               subplots=True)\n",
    "plt.ylabel('Mean Accuracy')\n",
    "\n",
    "plt.xlabel('Hyperparameter space')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "262bb97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.9740932642487047\n",
      "Test Accuracy:  0.9726775956284153\n"
     ]
    }
   ],
   "source": [
    " #let's get the predictions\n",
    "train_preds = search.predict(X_train)\n",
    "test_preds = search.predict(X_test)\n",
    "\n",
    "print('Train Accuracy: ', accuracy_score(y_train, train_preds))\n",
    "print('Test Accuracy: ', accuracy_score(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7ae810",
   "metadata": {},
   "source": [
    "## 3.Leave P Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c5916350",
   "metadata": {},
   "outputs": [],
   "source": [
    "lpo = LeavePOut(p=2)\n",
    "\n",
    "X_train_small = X_train.head(30)\n",
    "y_train_small = y_train.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8da99963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.1, 'penalty': 'l1'}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GridSearchCV(\n",
    "    logit,\n",
    "    param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=lpo, # LPOCV\n",
    "    refit=True, # refits best model to entire dataset\n",
    ")\n",
    "\n",
    "search = clf.fit(X_train_small, y_train_small)\n",
    "\n",
    "# best hyperparameters\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "aa884a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Hyperparameter space')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = pd.DataFrame(search.cv_results_)\n",
    "results = results[['params','mean_test_score','std_test_score']]\n",
    "results.sort_values(by='mean_test_score', \n",
    "                   ascending=False, inplace=True)\n",
    "results.reset_index(drop=True, inplace=True)\n",
    "results['mean_test_score'].plot(yerr=[results['std_test_score'],\n",
    "                                     results['std_test_score']],\n",
    "                               subplots=True)\n",
    "plt.ylabel('Mean Accuracy')\n",
    "\n",
    "plt.xlabel('Hyperparameter space')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d060985a",
   "metadata": {},
   "source": [
    "### Group Cross-Validation\n",
    "\n",
    "In this notebook we will demonstrate how to use 2 schemes of group cross-validation to first estimate the estimator generalization error, and then select the best hyperparameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "08a80a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import (GroupKFold,\n",
    "                                    LeaveOneGroupOut, cross_validate,\n",
    "                                    GridSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "78f63654",
   "metadata": {},
   "outputs": [],
   "source": [
    "brest_cancer_X, brest_cancer_y = load_breast_cancer(return_X_y=True)\n",
    "X = pd.DataFrame(brest_cancer_X)\n",
    "y = pd.Series(brest_cancer_y).map({0:1, 1:0})\n",
    "\n",
    "#add patients arbitrarily for demo\n",
    "patients_list = [p for p in range(10)]\n",
    "np.random.seed(1)\n",
    "X['patient'] = np.random.choice(patients_list, size=len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1e82a362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>patient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>1.0950</td>\n",
       "      <td>0.9053</td>\n",
       "      <td>8.589</td>\n",
       "      <td>153.40</td>\n",
       "      <td>0.006399</td>\n",
       "      <td>0.04904</td>\n",
       "      <td>0.05373</td>\n",
       "      <td>0.01587</td>\n",
       "      <td>0.03003</td>\n",
       "      <td>0.006193</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>0.5435</td>\n",
       "      <td>0.7339</td>\n",
       "      <td>3.398</td>\n",
       "      <td>74.08</td>\n",
       "      <td>0.005225</td>\n",
       "      <td>0.01308</td>\n",
       "      <td>0.01860</td>\n",
       "      <td>0.01340</td>\n",
       "      <td>0.01389</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>0.7456</td>\n",
       "      <td>0.7869</td>\n",
       "      <td>4.585</td>\n",
       "      <td>94.03</td>\n",
       "      <td>0.006150</td>\n",
       "      <td>0.04006</td>\n",
       "      <td>0.03832</td>\n",
       "      <td>0.02058</td>\n",
       "      <td>0.02250</td>\n",
       "      <td>0.004571</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>0.4956</td>\n",
       "      <td>1.1560</td>\n",
       "      <td>3.445</td>\n",
       "      <td>27.23</td>\n",
       "      <td>0.009110</td>\n",
       "      <td>0.07458</td>\n",
       "      <td>0.05661</td>\n",
       "      <td>0.01867</td>\n",
       "      <td>0.05963</td>\n",
       "      <td>0.009208</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>0.7572</td>\n",
       "      <td>0.7813</td>\n",
       "      <td>5.438</td>\n",
       "      <td>94.44</td>\n",
       "      <td>0.011490</td>\n",
       "      <td>0.02461</td>\n",
       "      <td>0.05688</td>\n",
       "      <td>0.01885</td>\n",
       "      <td>0.01756</td>\n",
       "      <td>0.005115</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1       2       3        4        5       6        7       8  \\\n",
       "0  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.3001  0.14710  0.2419   \n",
       "1  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.0869  0.07017  0.1812   \n",
       "2  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.1974  0.12790  0.2069   \n",
       "3  11.42  20.38   77.58   386.1  0.14250  0.28390  0.2414  0.10520  0.2597   \n",
       "4  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.1980  0.10430  0.1809   \n",
       "\n",
       "         9      10      11     12      13        14       15       16  \\\n",
       "0  0.07871  1.0950  0.9053  8.589  153.40  0.006399  0.04904  0.05373   \n",
       "1  0.05667  0.5435  0.7339  3.398   74.08  0.005225  0.01308  0.01860   \n",
       "2  0.05999  0.7456  0.7869  4.585   94.03  0.006150  0.04006  0.03832   \n",
       "3  0.09744  0.4956  1.1560  3.445   27.23  0.009110  0.07458  0.05661   \n",
       "4  0.05883  0.7572  0.7813  5.438   94.44  0.011490  0.02461  0.05688   \n",
       "\n",
       "        17       18        19     20     21      22      23      24      25  \\\n",
       "0  0.01587  0.03003  0.006193  25.38  17.33  184.60  2019.0  0.1622  0.6656   \n",
       "1  0.01340  0.01389  0.003532  24.99  23.41  158.80  1956.0  0.1238  0.1866   \n",
       "2  0.02058  0.02250  0.004571  23.57  25.53  152.50  1709.0  0.1444  0.4245   \n",
       "3  0.01867  0.05963  0.009208  14.91  26.50   98.87   567.7  0.2098  0.8663   \n",
       "4  0.01885  0.01756  0.005115  22.54  16.67  152.20  1575.0  0.1374  0.2050   \n",
       "\n",
       "       26      27      28       29  patient  \n",
       "0  0.7119  0.2654  0.4601  0.11890        5  \n",
       "1  0.2416  0.1860  0.2750  0.08902        8  \n",
       "2  0.4504  0.2430  0.3613  0.08758        9  \n",
       "3  0.6869  0.2575  0.6638  0.17300        5  \n",
       "4  0.4000  0.1625  0.2364  0.07678        0  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "dcc01185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X['patient'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "33559933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into a train and test set\n",
    "# this time, we leave data from 1 patient out\n",
    "X_train = X[X['patient']!=7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fcf101bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([  0,   1,   2,   3,   4,   5,   6,   8,   9,  10,\n",
       "            ...\n",
       "            557, 558, 559, 561, 562, 564, 565, 566, 567, 568],\n",
       "           dtype='int64', length=503)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "018076c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train = y.iloc[X_train.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ea96c1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# patient 7\n",
    "X_test = X[X['patient']==7]\n",
    "y_test = y.iloc[X_test.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a34926",
   "metadata": {},
   "source": [
    "## 1.Group K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d8465755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "logit = LogisticRegression(\n",
    "    penalty ='l2', C=10, solver='liblinear', random_state=4, max_iter=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ae9d318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouped k-Fols Cross Validation\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "#estimate Generlized error\n",
    "clf = cross_validate(logit, \n",
    "                    X_train.drop('patient', axis=1),\n",
    "                    y_train,\n",
    "                    scoring='accuracy',\n",
    "                    return_train_score=True,\n",
    "                    cv=gkf.split(X_train.drop('patient', axis=1),\n",
    "                                y_train, groups=X_train['patient']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9965b392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.94117647, 0.98214286, 0.94392523, 0.94444444, 0.96296296])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf['test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "40ac3a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group 1</th>\n",
       "      <th>group 2</th>\n",
       "      <th>group 3</th>\n",
       "      <th>group 4</th>\n",
       "      <th>group 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fit_time</th>\n",
       "      <td>0.014418</td>\n",
       "      <td>0.012572</td>\n",
       "      <td>0.011858</td>\n",
       "      <td>0.013090</td>\n",
       "      <td>0.014812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_time</th>\n",
       "      <td>0.004363</td>\n",
       "      <td>0.003553</td>\n",
       "      <td>0.003494</td>\n",
       "      <td>0.005231</td>\n",
       "      <td>0.004288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_score</th>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.982143</td>\n",
       "      <td>0.943925</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.962963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_score</th>\n",
       "      <td>0.970115</td>\n",
       "      <td>0.964194</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.967089</td>\n",
       "      <td>0.977215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              group 1   group 2   group 3   group 4   group 5\n",
       "fit_time     0.014418  0.012572  0.011858  0.013090  0.014812\n",
       "score_time   0.004363  0.003553  0.003494  0.005231  0.004288\n",
       "test_score   0.941176  0.982143  0.943925  0.944444  0.962963\n",
       "train_score  0.970115  0.964194  0.979798  0.967089  0.977215"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_list = []\n",
    "for i in range(1, len(clf.values())+2):\n",
    "    col_list.append(f\"group {i}\")\n",
    "col_list\n",
    "pd.DataFrame(clf.values(), columns=col_list,index = clf.keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "049c95e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean train set accuracy:  0.9716822186393209  +-  0.005934724812502724\n",
      "mean test set accuracy:  0.954930393756672  +-  0.01565426086525203\n"
     ]
    }
   ],
   "source": [
    "print('mean train set accuracy: ', np.mean(clf['train_score']), ' +- ', np.std(clf['train_score']))\n",
    "print('mean test set accuracy: ', np.mean(clf['test_score']), ' +- ', np.std(clf['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "231f53c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.9721669980119284\n",
      "Test Accuracy:  0.9545454545454546\n"
     ]
    }
   ],
   "source": [
    "# # fit the model to entire train set\n",
    "logit.fit(\n",
    "    X_train.drop('patient', axis=1), # drop the patient column, this is not a predictor\n",
    "    y_train)\n",
    "\n",
    "# let's get the predictions\n",
    "train_preds = logit.predict(X_train.drop('patient', axis=1))\n",
    "test_preds = logit.predict(X_test.drop('patient', axis=1))\n",
    "\n",
    "print('Train Accuracy: ', accuracy_score(y_train, train_preds))\n",
    "print('Test Accuracy: ', accuracy_score(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5885f7",
   "metadata": {},
   "source": [
    "## 2.Leave One Group Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "2ea543c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.94117647, 0.9787234 , 0.9137931 , 0.96226415, 0.94642857,\n",
       "       0.98076923, 0.98461538, 0.97959184, 0.98181818])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross-Validation\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# estimate generalization error\n",
    "clf =  cross_validate(\n",
    "    logit,\n",
    "    X_train.drop('patient', axis=1), # drop the patient column, this is not a predictor\n",
    "    y_train,\n",
    "    scoring='accuracy',\n",
    "    return_train_score=True,\n",
    "    cv=logo.split(X_train.drop('patient', axis=1), y_train, groups=X_train['patient']),\n",
    ")\n",
    "\n",
    "clf['test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a59e6c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean train set accuracy:  0.9681854158286476  +-  0.004935737590498333\n",
      "mean test set accuracy:  0.9632422594001433  +-  0.02317043011437911\n"
     ]
    }
   ],
   "source": [
    "print('mean train set accuracy: ', np.mean(clf['train_score']), ' +- ', np.std(clf['train_score']))\n",
    "print('mean test set accuracy: ', np.mean(clf['test_score']), ' +- ', np.std(clf['test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791dffef",
   "metadata": {},
   "source": [
    "## 3.Leave 1 Group Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "de28cdff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'penalty': 'l1'}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross-Validation\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# search\n",
    "clf = GridSearchCV(\n",
    "    logit,\n",
    "    param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=logo.split(X_train.drop('patient', axis=1), y_train, groups=X_train['patient']),\n",
    "    refit=True, # refits best model to entire dataset\n",
    ")\n",
    "\n",
    "search = clf.fit(\n",
    "    X_train.drop('patient', axis=1), # drop the patient column, this is not a predictor\n",
    "    y_train,\n",
    ")\n",
    "\n",
    "# best hyperparameters\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "49c0d0ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Hyperparameter space')"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = pd.DataFrame(search.cv_results_)[['params', 'mean_test_score', 'std_test_score']]\n",
    "results.sort_values(by='mean_test_score', ascending=False, inplace=True)\n",
    "\n",
    "results.reset_index(drop=True, inplace=True)\n",
    "\n",
    "results['mean_test_score'].plot(yerr=[results['std_test_score'], results['std_test_score']], subplots=True)\n",
    "\n",
    "plt.ylabel('Mean Accuracy')\n",
    "\n",
    "plt.xlabel('Hyperparameter space')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fd6cee",
   "metadata": {},
   "source": [
    "# Nested Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "365466bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>1.0950</td>\n",
       "      <td>0.9053</td>\n",
       "      <td>8.589</td>\n",
       "      <td>153.40</td>\n",
       "      <td>0.006399</td>\n",
       "      <td>0.04904</td>\n",
       "      <td>0.05373</td>\n",
       "      <td>0.01587</td>\n",
       "      <td>0.03003</td>\n",
       "      <td>0.006193</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>0.5435</td>\n",
       "      <td>0.7339</td>\n",
       "      <td>3.398</td>\n",
       "      <td>74.08</td>\n",
       "      <td>0.005225</td>\n",
       "      <td>0.01308</td>\n",
       "      <td>0.01860</td>\n",
       "      <td>0.01340</td>\n",
       "      <td>0.01389</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>0.7456</td>\n",
       "      <td>0.7869</td>\n",
       "      <td>4.585</td>\n",
       "      <td>94.03</td>\n",
       "      <td>0.006150</td>\n",
       "      <td>0.04006</td>\n",
       "      <td>0.03832</td>\n",
       "      <td>0.02058</td>\n",
       "      <td>0.02250</td>\n",
       "      <td>0.004571</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>0.4956</td>\n",
       "      <td>1.1560</td>\n",
       "      <td>3.445</td>\n",
       "      <td>27.23</td>\n",
       "      <td>0.009110</td>\n",
       "      <td>0.07458</td>\n",
       "      <td>0.05661</td>\n",
       "      <td>0.01867</td>\n",
       "      <td>0.05963</td>\n",
       "      <td>0.009208</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>0.7572</td>\n",
       "      <td>0.7813</td>\n",
       "      <td>5.438</td>\n",
       "      <td>94.44</td>\n",
       "      <td>0.011490</td>\n",
       "      <td>0.02461</td>\n",
       "      <td>0.05688</td>\n",
       "      <td>0.01885</td>\n",
       "      <td>0.01756</td>\n",
       "      <td>0.005115</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1       2       3        4        5       6        7       8   \\\n",
       "0  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.3001  0.14710  0.2419   \n",
       "1  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.0869  0.07017  0.1812   \n",
       "2  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.1974  0.12790  0.2069   \n",
       "3  11.42  20.38   77.58   386.1  0.14250  0.28390  0.2414  0.10520  0.2597   \n",
       "4  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.1980  0.10430  0.1809   \n",
       "\n",
       "        9       10      11     12      13        14       15       16  \\\n",
       "0  0.07871  1.0950  0.9053  8.589  153.40  0.006399  0.04904  0.05373   \n",
       "1  0.05667  0.5435  0.7339  3.398   74.08  0.005225  0.01308  0.01860   \n",
       "2  0.05999  0.7456  0.7869  4.585   94.03  0.006150  0.04006  0.03832   \n",
       "3  0.09744  0.4956  1.1560  3.445   27.23  0.009110  0.07458  0.05661   \n",
       "4  0.05883  0.7572  0.7813  5.438   94.44  0.011490  0.02461  0.05688   \n",
       "\n",
       "        17       18        19     20     21      22      23      24      25  \\\n",
       "0  0.01587  0.03003  0.006193  25.38  17.33  184.60  2019.0  0.1622  0.6656   \n",
       "1  0.01340  0.01389  0.003532  24.99  23.41  158.80  1956.0  0.1238  0.1866   \n",
       "2  0.02058  0.02250  0.004571  23.57  25.53  152.50  1709.0  0.1444  0.4245   \n",
       "3  0.01867  0.05963  0.009208  14.91  26.50   98.87   567.7  0.2098  0.8663   \n",
       "4  0.01885  0.01756  0.005115  22.54  16.67  152.20  1575.0  0.1374  0.2050   \n",
       "\n",
       "       26      27      28       29  \n",
       "0  0.7119  0.2654  0.4601  0.11890  \n",
       "1  0.2416  0.1860  0.2750  0.08902  \n",
       "2  0.4504  0.2430  0.3613  0.08758  \n",
       "3  0.6869  0.2575  0.6638  0.17300  \n",
       "4  0.4000  0.1625  0.2364  0.07678  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "breast_cancer_X, breast_cancer_y = load_breast_cancer(return_X_y=True)\n",
    "X = pd.DataFrame(breast_cancer_X)\n",
    "y = pd.Series(breast_cancer_y).map({0:1, 1:0})\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "7668f6cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((398, 30), (171, 30))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split dataset into a train and test set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "78f93f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>11.490</td>\n",
       "      <td>14.59</td>\n",
       "      <td>73.99</td>\n",
       "      <td>404.9</td>\n",
       "      <td>0.10460</td>\n",
       "      <td>0.08228</td>\n",
       "      <td>0.05308</td>\n",
       "      <td>0.01969</td>\n",
       "      <td>0.1779</td>\n",
       "      <td>0.06574</td>\n",
       "      <td>0.2034</td>\n",
       "      <td>1.1660</td>\n",
       "      <td>1.567</td>\n",
       "      <td>14.340</td>\n",
       "      <td>0.004957</td>\n",
       "      <td>0.021140</td>\n",
       "      <td>0.04156</td>\n",
       "      <td>0.008038</td>\n",
       "      <td>0.01843</td>\n",
       "      <td>0.003614</td>\n",
       "      <td>12.400</td>\n",
       "      <td>21.90</td>\n",
       "      <td>82.04</td>\n",
       "      <td>467.6</td>\n",
       "      <td>0.13520</td>\n",
       "      <td>0.20100</td>\n",
       "      <td>0.25960</td>\n",
       "      <td>0.07431</td>\n",
       "      <td>0.2941</td>\n",
       "      <td>0.09180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>10.490</td>\n",
       "      <td>18.61</td>\n",
       "      <td>66.86</td>\n",
       "      <td>334.3</td>\n",
       "      <td>0.10680</td>\n",
       "      <td>0.06678</td>\n",
       "      <td>0.02297</td>\n",
       "      <td>0.01780</td>\n",
       "      <td>0.1482</td>\n",
       "      <td>0.06600</td>\n",
       "      <td>0.1485</td>\n",
       "      <td>1.5630</td>\n",
       "      <td>1.035</td>\n",
       "      <td>10.080</td>\n",
       "      <td>0.008875</td>\n",
       "      <td>0.009362</td>\n",
       "      <td>0.01808</td>\n",
       "      <td>0.009199</td>\n",
       "      <td>0.01791</td>\n",
       "      <td>0.003317</td>\n",
       "      <td>11.060</td>\n",
       "      <td>24.54</td>\n",
       "      <td>70.76</td>\n",
       "      <td>375.4</td>\n",
       "      <td>0.14130</td>\n",
       "      <td>0.10440</td>\n",
       "      <td>0.08423</td>\n",
       "      <td>0.06528</td>\n",
       "      <td>0.2213</td>\n",
       "      <td>0.07842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>12.250</td>\n",
       "      <td>17.94</td>\n",
       "      <td>78.27</td>\n",
       "      <td>460.3</td>\n",
       "      <td>0.08654</td>\n",
       "      <td>0.06679</td>\n",
       "      <td>0.03885</td>\n",
       "      <td>0.02331</td>\n",
       "      <td>0.1970</td>\n",
       "      <td>0.06228</td>\n",
       "      <td>0.2200</td>\n",
       "      <td>0.9823</td>\n",
       "      <td>1.484</td>\n",
       "      <td>16.510</td>\n",
       "      <td>0.005518</td>\n",
       "      <td>0.015620</td>\n",
       "      <td>0.01994</td>\n",
       "      <td>0.007924</td>\n",
       "      <td>0.01799</td>\n",
       "      <td>0.002484</td>\n",
       "      <td>13.590</td>\n",
       "      <td>25.22</td>\n",
       "      <td>86.60</td>\n",
       "      <td>564.2</td>\n",
       "      <td>0.12170</td>\n",
       "      <td>0.17880</td>\n",
       "      <td>0.19430</td>\n",
       "      <td>0.08211</td>\n",
       "      <td>0.3113</td>\n",
       "      <td>0.08132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>18.310</td>\n",
       "      <td>18.58</td>\n",
       "      <td>118.60</td>\n",
       "      <td>1041.0</td>\n",
       "      <td>0.08588</td>\n",
       "      <td>0.08468</td>\n",
       "      <td>0.08169</td>\n",
       "      <td>0.05814</td>\n",
       "      <td>0.1621</td>\n",
       "      <td>0.05425</td>\n",
       "      <td>0.2577</td>\n",
       "      <td>0.4757</td>\n",
       "      <td>1.817</td>\n",
       "      <td>28.920</td>\n",
       "      <td>0.002866</td>\n",
       "      <td>0.009181</td>\n",
       "      <td>0.01412</td>\n",
       "      <td>0.006719</td>\n",
       "      <td>0.01069</td>\n",
       "      <td>0.001087</td>\n",
       "      <td>21.310</td>\n",
       "      <td>26.36</td>\n",
       "      <td>139.20</td>\n",
       "      <td>1410.0</td>\n",
       "      <td>0.12340</td>\n",
       "      <td>0.24450</td>\n",
       "      <td>0.35380</td>\n",
       "      <td>0.15710</td>\n",
       "      <td>0.3206</td>\n",
       "      <td>0.06938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>6.981</td>\n",
       "      <td>13.43</td>\n",
       "      <td>43.79</td>\n",
       "      <td>143.5</td>\n",
       "      <td>0.11700</td>\n",
       "      <td>0.07568</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>0.07818</td>\n",
       "      <td>0.2241</td>\n",
       "      <td>1.5080</td>\n",
       "      <td>1.553</td>\n",
       "      <td>9.833</td>\n",
       "      <td>0.010190</td>\n",
       "      <td>0.010840</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.02659</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>7.930</td>\n",
       "      <td>19.54</td>\n",
       "      <td>50.41</td>\n",
       "      <td>185.2</td>\n",
       "      <td>0.15840</td>\n",
       "      <td>0.12020</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.2932</td>\n",
       "      <td>0.09382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>18.810</td>\n",
       "      <td>19.98</td>\n",
       "      <td>120.90</td>\n",
       "      <td>1102.0</td>\n",
       "      <td>0.08923</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>0.08020</td>\n",
       "      <td>0.05843</td>\n",
       "      <td>0.1550</td>\n",
       "      <td>0.04996</td>\n",
       "      <td>0.3283</td>\n",
       "      <td>0.8280</td>\n",
       "      <td>2.363</td>\n",
       "      <td>36.740</td>\n",
       "      <td>0.007571</td>\n",
       "      <td>0.011140</td>\n",
       "      <td>0.02623</td>\n",
       "      <td>0.014630</td>\n",
       "      <td>0.01930</td>\n",
       "      <td>0.001676</td>\n",
       "      <td>19.960</td>\n",
       "      <td>24.30</td>\n",
       "      <td>129.00</td>\n",
       "      <td>1236.0</td>\n",
       "      <td>0.12430</td>\n",
       "      <td>0.11600</td>\n",
       "      <td>0.22100</td>\n",
       "      <td>0.12940</td>\n",
       "      <td>0.2567</td>\n",
       "      <td>0.05737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12.460</td>\n",
       "      <td>24.04</td>\n",
       "      <td>83.97</td>\n",
       "      <td>475.9</td>\n",
       "      <td>0.11860</td>\n",
       "      <td>0.23960</td>\n",
       "      <td>0.22730</td>\n",
       "      <td>0.08543</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>0.08243</td>\n",
       "      <td>0.2976</td>\n",
       "      <td>1.5990</td>\n",
       "      <td>2.039</td>\n",
       "      <td>23.940</td>\n",
       "      <td>0.007149</td>\n",
       "      <td>0.072170</td>\n",
       "      <td>0.07743</td>\n",
       "      <td>0.014320</td>\n",
       "      <td>0.01789</td>\n",
       "      <td>0.010080</td>\n",
       "      <td>15.090</td>\n",
       "      <td>40.68</td>\n",
       "      <td>97.65</td>\n",
       "      <td>711.4</td>\n",
       "      <td>0.18530</td>\n",
       "      <td>1.05800</td>\n",
       "      <td>1.10500</td>\n",
       "      <td>0.22100</td>\n",
       "      <td>0.4366</td>\n",
       "      <td>0.20750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>9.436</td>\n",
       "      <td>18.32</td>\n",
       "      <td>59.82</td>\n",
       "      <td>278.6</td>\n",
       "      <td>0.10090</td>\n",
       "      <td>0.05956</td>\n",
       "      <td>0.02710</td>\n",
       "      <td>0.01406</td>\n",
       "      <td>0.1506</td>\n",
       "      <td>0.06959</td>\n",
       "      <td>0.5079</td>\n",
       "      <td>1.2470</td>\n",
       "      <td>3.267</td>\n",
       "      <td>30.480</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.008982</td>\n",
       "      <td>0.02348</td>\n",
       "      <td>0.006565</td>\n",
       "      <td>0.01942</td>\n",
       "      <td>0.002713</td>\n",
       "      <td>12.020</td>\n",
       "      <td>25.02</td>\n",
       "      <td>75.79</td>\n",
       "      <td>439.6</td>\n",
       "      <td>0.13330</td>\n",
       "      <td>0.10490</td>\n",
       "      <td>0.11440</td>\n",
       "      <td>0.05052</td>\n",
       "      <td>0.2454</td>\n",
       "      <td>0.08136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>9.720</td>\n",
       "      <td>18.22</td>\n",
       "      <td>60.73</td>\n",
       "      <td>288.1</td>\n",
       "      <td>0.06950</td>\n",
       "      <td>0.02344</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1653</td>\n",
       "      <td>0.06447</td>\n",
       "      <td>0.3539</td>\n",
       "      <td>4.8850</td>\n",
       "      <td>2.230</td>\n",
       "      <td>21.690</td>\n",
       "      <td>0.001713</td>\n",
       "      <td>0.006736</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.03799</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>9.968</td>\n",
       "      <td>20.83</td>\n",
       "      <td>62.25</td>\n",
       "      <td>303.8</td>\n",
       "      <td>0.07117</td>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1909</td>\n",
       "      <td>0.06559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>11.510</td>\n",
       "      <td>23.93</td>\n",
       "      <td>74.52</td>\n",
       "      <td>403.5</td>\n",
       "      <td>0.09261</td>\n",
       "      <td>0.10210</td>\n",
       "      <td>0.11120</td>\n",
       "      <td>0.04105</td>\n",
       "      <td>0.1388</td>\n",
       "      <td>0.06570</td>\n",
       "      <td>0.2388</td>\n",
       "      <td>2.9040</td>\n",
       "      <td>1.936</td>\n",
       "      <td>16.970</td>\n",
       "      <td>0.008200</td>\n",
       "      <td>0.029820</td>\n",
       "      <td>0.05738</td>\n",
       "      <td>0.012670</td>\n",
       "      <td>0.01488</td>\n",
       "      <td>0.004738</td>\n",
       "      <td>12.480</td>\n",
       "      <td>37.16</td>\n",
       "      <td>82.28</td>\n",
       "      <td>474.2</td>\n",
       "      <td>0.12980</td>\n",
       "      <td>0.25170</td>\n",
       "      <td>0.36300</td>\n",
       "      <td>0.09653</td>\n",
       "      <td>0.2112</td>\n",
       "      <td>0.08732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>398 rows  30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1       2       3        4        5        6        7   \\\n",
       "478  11.490  14.59   73.99   404.9  0.10460  0.08228  0.05308  0.01969   \n",
       "303  10.490  18.61   66.86   334.3  0.10680  0.06678  0.02297  0.01780   \n",
       "155  12.250  17.94   78.27   460.3  0.08654  0.06679  0.03885  0.02331   \n",
       "186  18.310  18.58  118.60  1041.0  0.08588  0.08468  0.08169  0.05814   \n",
       "101   6.981  13.43   43.79   143.5  0.11700  0.07568  0.00000  0.00000   \n",
       "..      ...    ...     ...     ...      ...      ...      ...      ...   \n",
       "277  18.810  19.98  120.90  1102.0  0.08923  0.05884  0.08020  0.05843   \n",
       "9    12.460  24.04   83.97   475.9  0.11860  0.23960  0.22730  0.08543   \n",
       "359   9.436  18.32   59.82   278.6  0.10090  0.05956  0.02710  0.01406   \n",
       "192   9.720  18.22   60.73   288.1  0.06950  0.02344  0.00000  0.00000   \n",
       "559  11.510  23.93   74.52   403.5  0.09261  0.10210  0.11120  0.04105   \n",
       "\n",
       "         8        9       10      11     12      13        14        15  \\\n",
       "478  0.1779  0.06574  0.2034  1.1660  1.567  14.340  0.004957  0.021140   \n",
       "303  0.1482  0.06600  0.1485  1.5630  1.035  10.080  0.008875  0.009362   \n",
       "155  0.1970  0.06228  0.2200  0.9823  1.484  16.510  0.005518  0.015620   \n",
       "186  0.1621  0.05425  0.2577  0.4757  1.817  28.920  0.002866  0.009181   \n",
       "101  0.1930  0.07818  0.2241  1.5080  1.553   9.833  0.010190  0.010840   \n",
       "..      ...      ...     ...     ...    ...     ...       ...       ...   \n",
       "277  0.1550  0.04996  0.3283  0.8280  2.363  36.740  0.007571  0.011140   \n",
       "9    0.2030  0.08243  0.2976  1.5990  2.039  23.940  0.007149  0.072170   \n",
       "359  0.1506  0.06959  0.5079  1.2470  3.267  30.480  0.006836  0.008982   \n",
       "192  0.1653  0.06447  0.3539  4.8850  2.230  21.690  0.001713  0.006736   \n",
       "559  0.1388  0.06570  0.2388  2.9040  1.936  16.970  0.008200  0.029820   \n",
       "\n",
       "          16        17       18        19      20     21      22      23  \\\n",
       "478  0.04156  0.008038  0.01843  0.003614  12.400  21.90   82.04   467.6   \n",
       "303  0.01808  0.009199  0.01791  0.003317  11.060  24.54   70.76   375.4   \n",
       "155  0.01994  0.007924  0.01799  0.002484  13.590  25.22   86.60   564.2   \n",
       "186  0.01412  0.006719  0.01069  0.001087  21.310  26.36  139.20  1410.0   \n",
       "101  0.00000  0.000000  0.02659  0.004100   7.930  19.54   50.41   185.2   \n",
       "..       ...       ...      ...       ...     ...    ...     ...     ...   \n",
       "277  0.02623  0.014630  0.01930  0.001676  19.960  24.30  129.00  1236.0   \n",
       "9    0.07743  0.014320  0.01789  0.010080  15.090  40.68   97.65   711.4   \n",
       "359  0.02348  0.006565  0.01942  0.002713  12.020  25.02   75.79   439.6   \n",
       "192  0.00000  0.000000  0.03799  0.001688   9.968  20.83   62.25   303.8   \n",
       "559  0.05738  0.012670  0.01488  0.004738  12.480  37.16   82.28   474.2   \n",
       "\n",
       "          24       25       26       27      28       29  \n",
       "478  0.13520  0.20100  0.25960  0.07431  0.2941  0.09180  \n",
       "303  0.14130  0.10440  0.08423  0.06528  0.2213  0.07842  \n",
       "155  0.12170  0.17880  0.19430  0.08211  0.3113  0.08132  \n",
       "186  0.12340  0.24450  0.35380  0.15710  0.3206  0.06938  \n",
       "101  0.15840  0.12020  0.00000  0.00000  0.2932  0.09382  \n",
       "..       ...      ...      ...      ...     ...      ...  \n",
       "277  0.12430  0.11600  0.22100  0.12940  0.2567  0.05737  \n",
       "9    0.18530  1.05800  1.10500  0.22100  0.4366  0.20750  \n",
       "359  0.13330  0.10490  0.11440  0.05052  0.2454  0.08136  \n",
       "192  0.07117  0.02729  0.00000  0.00000  0.1909  0.06559  \n",
       "559  0.12980  0.25170  0.36300  0.09653  0.2112  0.08732  \n",
       "\n",
       "[398 rows x 30 columns]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3e3123eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5bc4e88f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.490</td>\n",
       "      <td>14.59</td>\n",
       "      <td>73.99</td>\n",
       "      <td>404.9</td>\n",
       "      <td>0.10460</td>\n",
       "      <td>0.08228</td>\n",
       "      <td>0.05308</td>\n",
       "      <td>0.01969</td>\n",
       "      <td>0.1779</td>\n",
       "      <td>0.06574</td>\n",
       "      <td>0.2034</td>\n",
       "      <td>1.1660</td>\n",
       "      <td>1.567</td>\n",
       "      <td>14.340</td>\n",
       "      <td>0.004957</td>\n",
       "      <td>0.021140</td>\n",
       "      <td>0.04156</td>\n",
       "      <td>0.008038</td>\n",
       "      <td>0.01843</td>\n",
       "      <td>0.003614</td>\n",
       "      <td>12.400</td>\n",
       "      <td>21.90</td>\n",
       "      <td>82.04</td>\n",
       "      <td>467.6</td>\n",
       "      <td>0.13520</td>\n",
       "      <td>0.20100</td>\n",
       "      <td>0.25960</td>\n",
       "      <td>0.07431</td>\n",
       "      <td>0.2941</td>\n",
       "      <td>0.09180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.490</td>\n",
       "      <td>18.61</td>\n",
       "      <td>66.86</td>\n",
       "      <td>334.3</td>\n",
       "      <td>0.10680</td>\n",
       "      <td>0.06678</td>\n",
       "      <td>0.02297</td>\n",
       "      <td>0.01780</td>\n",
       "      <td>0.1482</td>\n",
       "      <td>0.06600</td>\n",
       "      <td>0.1485</td>\n",
       "      <td>1.5630</td>\n",
       "      <td>1.035</td>\n",
       "      <td>10.080</td>\n",
       "      <td>0.008875</td>\n",
       "      <td>0.009362</td>\n",
       "      <td>0.01808</td>\n",
       "      <td>0.009199</td>\n",
       "      <td>0.01791</td>\n",
       "      <td>0.003317</td>\n",
       "      <td>11.060</td>\n",
       "      <td>24.54</td>\n",
       "      <td>70.76</td>\n",
       "      <td>375.4</td>\n",
       "      <td>0.14130</td>\n",
       "      <td>0.10440</td>\n",
       "      <td>0.08423</td>\n",
       "      <td>0.06528</td>\n",
       "      <td>0.2213</td>\n",
       "      <td>0.07842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.250</td>\n",
       "      <td>17.94</td>\n",
       "      <td>78.27</td>\n",
       "      <td>460.3</td>\n",
       "      <td>0.08654</td>\n",
       "      <td>0.06679</td>\n",
       "      <td>0.03885</td>\n",
       "      <td>0.02331</td>\n",
       "      <td>0.1970</td>\n",
       "      <td>0.06228</td>\n",
       "      <td>0.2200</td>\n",
       "      <td>0.9823</td>\n",
       "      <td>1.484</td>\n",
       "      <td>16.510</td>\n",
       "      <td>0.005518</td>\n",
       "      <td>0.015620</td>\n",
       "      <td>0.01994</td>\n",
       "      <td>0.007924</td>\n",
       "      <td>0.01799</td>\n",
       "      <td>0.002484</td>\n",
       "      <td>13.590</td>\n",
       "      <td>25.22</td>\n",
       "      <td>86.60</td>\n",
       "      <td>564.2</td>\n",
       "      <td>0.12170</td>\n",
       "      <td>0.17880</td>\n",
       "      <td>0.19430</td>\n",
       "      <td>0.08211</td>\n",
       "      <td>0.3113</td>\n",
       "      <td>0.08132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18.310</td>\n",
       "      <td>18.58</td>\n",
       "      <td>118.60</td>\n",
       "      <td>1041.0</td>\n",
       "      <td>0.08588</td>\n",
       "      <td>0.08468</td>\n",
       "      <td>0.08169</td>\n",
       "      <td>0.05814</td>\n",
       "      <td>0.1621</td>\n",
       "      <td>0.05425</td>\n",
       "      <td>0.2577</td>\n",
       "      <td>0.4757</td>\n",
       "      <td>1.817</td>\n",
       "      <td>28.920</td>\n",
       "      <td>0.002866</td>\n",
       "      <td>0.009181</td>\n",
       "      <td>0.01412</td>\n",
       "      <td>0.006719</td>\n",
       "      <td>0.01069</td>\n",
       "      <td>0.001087</td>\n",
       "      <td>21.310</td>\n",
       "      <td>26.36</td>\n",
       "      <td>139.20</td>\n",
       "      <td>1410.0</td>\n",
       "      <td>0.12340</td>\n",
       "      <td>0.24450</td>\n",
       "      <td>0.35380</td>\n",
       "      <td>0.15710</td>\n",
       "      <td>0.3206</td>\n",
       "      <td>0.06938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.981</td>\n",
       "      <td>13.43</td>\n",
       "      <td>43.79</td>\n",
       "      <td>143.5</td>\n",
       "      <td>0.11700</td>\n",
       "      <td>0.07568</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>0.07818</td>\n",
       "      <td>0.2241</td>\n",
       "      <td>1.5080</td>\n",
       "      <td>1.553</td>\n",
       "      <td>9.833</td>\n",
       "      <td>0.010190</td>\n",
       "      <td>0.010840</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.02659</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>7.930</td>\n",
       "      <td>19.54</td>\n",
       "      <td>50.41</td>\n",
       "      <td>185.2</td>\n",
       "      <td>0.15840</td>\n",
       "      <td>0.12020</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.2932</td>\n",
       "      <td>0.09382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>18.810</td>\n",
       "      <td>19.98</td>\n",
       "      <td>120.90</td>\n",
       "      <td>1102.0</td>\n",
       "      <td>0.08923</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>0.08020</td>\n",
       "      <td>0.05843</td>\n",
       "      <td>0.1550</td>\n",
       "      <td>0.04996</td>\n",
       "      <td>0.3283</td>\n",
       "      <td>0.8280</td>\n",
       "      <td>2.363</td>\n",
       "      <td>36.740</td>\n",
       "      <td>0.007571</td>\n",
       "      <td>0.011140</td>\n",
       "      <td>0.02623</td>\n",
       "      <td>0.014630</td>\n",
       "      <td>0.01930</td>\n",
       "      <td>0.001676</td>\n",
       "      <td>19.960</td>\n",
       "      <td>24.30</td>\n",
       "      <td>129.00</td>\n",
       "      <td>1236.0</td>\n",
       "      <td>0.12430</td>\n",
       "      <td>0.11600</td>\n",
       "      <td>0.22100</td>\n",
       "      <td>0.12940</td>\n",
       "      <td>0.2567</td>\n",
       "      <td>0.05737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>12.460</td>\n",
       "      <td>24.04</td>\n",
       "      <td>83.97</td>\n",
       "      <td>475.9</td>\n",
       "      <td>0.11860</td>\n",
       "      <td>0.23960</td>\n",
       "      <td>0.22730</td>\n",
       "      <td>0.08543</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>0.08243</td>\n",
       "      <td>0.2976</td>\n",
       "      <td>1.5990</td>\n",
       "      <td>2.039</td>\n",
       "      <td>23.940</td>\n",
       "      <td>0.007149</td>\n",
       "      <td>0.072170</td>\n",
       "      <td>0.07743</td>\n",
       "      <td>0.014320</td>\n",
       "      <td>0.01789</td>\n",
       "      <td>0.010080</td>\n",
       "      <td>15.090</td>\n",
       "      <td>40.68</td>\n",
       "      <td>97.65</td>\n",
       "      <td>711.4</td>\n",
       "      <td>0.18530</td>\n",
       "      <td>1.05800</td>\n",
       "      <td>1.10500</td>\n",
       "      <td>0.22100</td>\n",
       "      <td>0.4366</td>\n",
       "      <td>0.20750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>9.436</td>\n",
       "      <td>18.32</td>\n",
       "      <td>59.82</td>\n",
       "      <td>278.6</td>\n",
       "      <td>0.10090</td>\n",
       "      <td>0.05956</td>\n",
       "      <td>0.02710</td>\n",
       "      <td>0.01406</td>\n",
       "      <td>0.1506</td>\n",
       "      <td>0.06959</td>\n",
       "      <td>0.5079</td>\n",
       "      <td>1.2470</td>\n",
       "      <td>3.267</td>\n",
       "      <td>30.480</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.008982</td>\n",
       "      <td>0.02348</td>\n",
       "      <td>0.006565</td>\n",
       "      <td>0.01942</td>\n",
       "      <td>0.002713</td>\n",
       "      <td>12.020</td>\n",
       "      <td>25.02</td>\n",
       "      <td>75.79</td>\n",
       "      <td>439.6</td>\n",
       "      <td>0.13330</td>\n",
       "      <td>0.10490</td>\n",
       "      <td>0.11440</td>\n",
       "      <td>0.05052</td>\n",
       "      <td>0.2454</td>\n",
       "      <td>0.08136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>9.720</td>\n",
       "      <td>18.22</td>\n",
       "      <td>60.73</td>\n",
       "      <td>288.1</td>\n",
       "      <td>0.06950</td>\n",
       "      <td>0.02344</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1653</td>\n",
       "      <td>0.06447</td>\n",
       "      <td>0.3539</td>\n",
       "      <td>4.8850</td>\n",
       "      <td>2.230</td>\n",
       "      <td>21.690</td>\n",
       "      <td>0.001713</td>\n",
       "      <td>0.006736</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.03799</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>9.968</td>\n",
       "      <td>20.83</td>\n",
       "      <td>62.25</td>\n",
       "      <td>303.8</td>\n",
       "      <td>0.07117</td>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1909</td>\n",
       "      <td>0.06559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>11.510</td>\n",
       "      <td>23.93</td>\n",
       "      <td>74.52</td>\n",
       "      <td>403.5</td>\n",
       "      <td>0.09261</td>\n",
       "      <td>0.10210</td>\n",
       "      <td>0.11120</td>\n",
       "      <td>0.04105</td>\n",
       "      <td>0.1388</td>\n",
       "      <td>0.06570</td>\n",
       "      <td>0.2388</td>\n",
       "      <td>2.9040</td>\n",
       "      <td>1.936</td>\n",
       "      <td>16.970</td>\n",
       "      <td>0.008200</td>\n",
       "      <td>0.029820</td>\n",
       "      <td>0.05738</td>\n",
       "      <td>0.012670</td>\n",
       "      <td>0.01488</td>\n",
       "      <td>0.004738</td>\n",
       "      <td>12.480</td>\n",
       "      <td>37.16</td>\n",
       "      <td>82.28</td>\n",
       "      <td>474.2</td>\n",
       "      <td>0.12980</td>\n",
       "      <td>0.25170</td>\n",
       "      <td>0.36300</td>\n",
       "      <td>0.09653</td>\n",
       "      <td>0.2112</td>\n",
       "      <td>0.08732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>398 rows  30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1       2       3        4        5        6        7   \\\n",
       "0    11.490  14.59   73.99   404.9  0.10460  0.08228  0.05308  0.01969   \n",
       "1    10.490  18.61   66.86   334.3  0.10680  0.06678  0.02297  0.01780   \n",
       "2    12.250  17.94   78.27   460.3  0.08654  0.06679  0.03885  0.02331   \n",
       "3    18.310  18.58  118.60  1041.0  0.08588  0.08468  0.08169  0.05814   \n",
       "4     6.981  13.43   43.79   143.5  0.11700  0.07568  0.00000  0.00000   \n",
       "..      ...    ...     ...     ...      ...      ...      ...      ...   \n",
       "393  18.810  19.98  120.90  1102.0  0.08923  0.05884  0.08020  0.05843   \n",
       "394  12.460  24.04   83.97   475.9  0.11860  0.23960  0.22730  0.08543   \n",
       "395   9.436  18.32   59.82   278.6  0.10090  0.05956  0.02710  0.01406   \n",
       "396   9.720  18.22   60.73   288.1  0.06950  0.02344  0.00000  0.00000   \n",
       "397  11.510  23.93   74.52   403.5  0.09261  0.10210  0.11120  0.04105   \n",
       "\n",
       "         8        9       10      11     12      13        14        15  \\\n",
       "0    0.1779  0.06574  0.2034  1.1660  1.567  14.340  0.004957  0.021140   \n",
       "1    0.1482  0.06600  0.1485  1.5630  1.035  10.080  0.008875  0.009362   \n",
       "2    0.1970  0.06228  0.2200  0.9823  1.484  16.510  0.005518  0.015620   \n",
       "3    0.1621  0.05425  0.2577  0.4757  1.817  28.920  0.002866  0.009181   \n",
       "4    0.1930  0.07818  0.2241  1.5080  1.553   9.833  0.010190  0.010840   \n",
       "..      ...      ...     ...     ...    ...     ...       ...       ...   \n",
       "393  0.1550  0.04996  0.3283  0.8280  2.363  36.740  0.007571  0.011140   \n",
       "394  0.2030  0.08243  0.2976  1.5990  2.039  23.940  0.007149  0.072170   \n",
       "395  0.1506  0.06959  0.5079  1.2470  3.267  30.480  0.006836  0.008982   \n",
       "396  0.1653  0.06447  0.3539  4.8850  2.230  21.690  0.001713  0.006736   \n",
       "397  0.1388  0.06570  0.2388  2.9040  1.936  16.970  0.008200  0.029820   \n",
       "\n",
       "          16        17       18        19      20     21      22      23  \\\n",
       "0    0.04156  0.008038  0.01843  0.003614  12.400  21.90   82.04   467.6   \n",
       "1    0.01808  0.009199  0.01791  0.003317  11.060  24.54   70.76   375.4   \n",
       "2    0.01994  0.007924  0.01799  0.002484  13.590  25.22   86.60   564.2   \n",
       "3    0.01412  0.006719  0.01069  0.001087  21.310  26.36  139.20  1410.0   \n",
       "4    0.00000  0.000000  0.02659  0.004100   7.930  19.54   50.41   185.2   \n",
       "..       ...       ...      ...       ...     ...    ...     ...     ...   \n",
       "393  0.02623  0.014630  0.01930  0.001676  19.960  24.30  129.00  1236.0   \n",
       "394  0.07743  0.014320  0.01789  0.010080  15.090  40.68   97.65   711.4   \n",
       "395  0.02348  0.006565  0.01942  0.002713  12.020  25.02   75.79   439.6   \n",
       "396  0.00000  0.000000  0.03799  0.001688   9.968  20.83   62.25   303.8   \n",
       "397  0.05738  0.012670  0.01488  0.004738  12.480  37.16   82.28   474.2   \n",
       "\n",
       "          24       25       26       27      28       29  \n",
       "0    0.13520  0.20100  0.25960  0.07431  0.2941  0.09180  \n",
       "1    0.14130  0.10440  0.08423  0.06528  0.2213  0.07842  \n",
       "2    0.12170  0.17880  0.19430  0.08211  0.3113  0.08132  \n",
       "3    0.12340  0.24450  0.35380  0.15710  0.3206  0.06938  \n",
       "4    0.15840  0.12020  0.00000  0.00000  0.2932  0.09382  \n",
       "..       ...      ...      ...      ...     ...      ...  \n",
       "393  0.12430  0.11600  0.22100  0.12940  0.2567  0.05737  \n",
       "394  0.18530  1.05800  1.10500  0.22100  0.4366  0.20750  \n",
       "395  0.13330  0.10490  0.11440  0.05052  0.2454  0.08136  \n",
       "396  0.07117  0.02729  0.00000  0.00000  0.1909  0.06559  \n",
       "397  0.12980  0.25170  0.36300  0.09653  0.2112  0.08732  \n",
       "\n",
       "[398 rows x 30 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a63ce4",
   "metadata": {},
   "source": [
    "## 1. Netsed Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "1251fb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_cross_val(model, grid):\n",
    "\n",
    "    # configure the outer loop cross-validation procedure\n",
    "    cv_outer = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "    # configure the inner loop cross-validation procedure\n",
    "    cv_inner = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "    # enumerate splits\n",
    "    outer_results = list()\n",
    "    inner_results = list()\n",
    "\n",
    "    for train_ix, test_ix in cv_outer.split(X_train):\n",
    "\n",
    "        # split data\n",
    "        xtrain, xtest = X_train.loc[train_ix, :], X_train.loc[test_ix, :]\n",
    "        ytrain, ytest = y_train[train_ix], y_train[test_ix]\n",
    "\n",
    "        # define search\n",
    "        search = GridSearchCV(\n",
    "            model, grid, scoring='accuracy', cv=cv_inner, refit=True)\n",
    "\n",
    "        # execute search\n",
    "        search.fit(xtrain, ytrain)\n",
    "\n",
    "        # evaluate model on the hold out dataset\n",
    "        yhat = search.predict(xtest)\n",
    "\n",
    "        # evaluate the model\n",
    "        accuracy = accuracy_score(ytest, yhat)\n",
    "\n",
    "        # store the result\n",
    "        outer_results.append(accuracy)\n",
    "        \n",
    "        inner_results.append(search.best_score_)\n",
    "\n",
    "        # report progress\n",
    "        print(' >> accuracy_outer=%.3f, accuracy_inner=%.3f, cfg=%s' %\n",
    "              (accuracy, search.best_score_, search.best_params_))\n",
    "\n",
    "    # summarize the estimated performance of the model\n",
    "    print()\n",
    "    print('accuracy_outer: %.3f +- %.3f' %\n",
    "          (np.mean(outer_results), np.std(outer_results)))\n",
    "    print('accuracy_inner: %.3f +- %.3f' %\n",
    "          (np.mean(inner_results), np.std(inner_results)))\n",
    "\n",
    "    return search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454dbf36",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e5646c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(penalty='l2',\n",
    "                          C=1, \n",
    "                          solver='liblinear', random_state=4,\n",
    "                          max_iter=10000)\n",
    "\n",
    "#hyperparameter space\n",
    "logit_param = dict(penalty=['l1','l2'],\n",
    "                  C=[0.1, 1, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "636699ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >> accuracy_outer=0.975, accuracy_inner=0.950, cfg={'C': 10, 'penalty': 'l1'}\n",
      " >> accuracy_outer=0.963, accuracy_inner=0.947, cfg={'C': 10, 'penalty': 'l1'}\n",
      " >> accuracy_outer=0.975, accuracy_inner=0.959, cfg={'C': 10, 'penalty': 'l1'}\n",
      " >> accuracy_outer=0.962, accuracy_inner=0.959, cfg={'C': 10, 'penalty': 'l1'}\n",
      " >> accuracy_outer=0.937, accuracy_inner=0.959, cfg={'C': 10, 'penalty': 'l1'}\n",
      "\n",
      "accuracy_outer: 0.962 +- 0.014\n",
      "accuracy_inner: 0.955 +- 0.005\n"
     ]
    }
   ],
   "source": [
    "logit_search = nested_cross_val(logit, logit_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "a8510120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.9748743718592965\n",
      "Test accuracy:  0.9707602339181286\n"
     ]
    }
   ],
   "source": [
    "# let's get the predictions\n",
    "\n",
    "X_train_preds = logit_search.predict(X_train)\n",
    "X_test_preds = logit_search.predict(X_test)\n",
    "\n",
    "# let's examine the accuracy\n",
    "print('Train accuracy: ', accuracy_score(y_train, X_train_preds))\n",
    "print('Test accuracy: ', accuracy_score(y_test, X_test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2316824e",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "71c203f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_param = dict(\n",
    "    n_estimators=[10, 50, 100, 200],\n",
    "    min_samples_split=[0.1, 0.3, 0.5, 1.0],\n",
    "    max_depth=[1,2,3,None],\n",
    "    )\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100,\n",
    "                           min_samples_split=2,\n",
    "                           max_depth=3,\n",
    "                           random_state=0,\n",
    "                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "62caceef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >> accuracy_outer=0.963, accuracy_inner=0.946, cfg={'max_depth': None, 'min_samples_split': 0.1, 'n_estimators': 200}\n",
      " >> accuracy_outer=0.950, accuracy_inner=0.953, cfg={'max_depth': 2, 'min_samples_split': 0.1, 'n_estimators': 10}\n",
      " >> accuracy_outer=0.938, accuracy_inner=0.956, cfg={'max_depth': None, 'min_samples_split': 0.1, 'n_estimators': 100}\n",
      " >> accuracy_outer=0.987, accuracy_inner=0.934, cfg={'max_depth': 2, 'min_samples_split': 0.1, 'n_estimators': 50}\n",
      " >> accuracy_outer=0.911, accuracy_inner=0.959, cfg={'max_depth': 2, 'min_samples_split': 0.1, 'n_estimators': 200}\n",
      "\n",
      "accuracy_outer: 0.950 +- 0.025\n",
      "accuracy_inner: 0.950 +- 0.009\n"
     ]
    }
   ],
   "source": [
    "rf_search = nested_cross_val(rf, rf_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd03ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
